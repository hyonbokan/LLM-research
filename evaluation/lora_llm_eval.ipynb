{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import traceback\n",
    "from threading import Lock\n",
    "\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "new_model = \"/home/hb/dataset_bgp/finetuned_models/LLaMA3-8B-analysis-5k-no_4bit_paged_adam32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    new_model\n",
    ")\n",
    "\n",
    "# Merge LoRA weights into base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Ensure we have a valid pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=712\n",
    ")\n",
    "print(\"[INFO] Model, tokenizer, and pipeline are loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt, pipe_ref):\n",
    "    \"\"\"\n",
    "    Generates text from the LLM using the pipeline.\n",
    "    \"\"\"\n",
    "    generation_kwargs = dict(\n",
    "        max_new_tokens=712,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.0,\n",
    "        eos_token_id=pipe_ref.tokenizer.eos_token_id,\n",
    "        pad_token_id=pipe_ref.tokenizer.pad_token_id,\n",
    "    )\n",
    "    result = pipe_ref(prompt, **generation_kwargs)\n",
    "    return result[0][\"generated_text\"] if result else None\n",
    "\n",
    "def extract_code_from_reply(llm_output):\n",
    "    \"\"\"\n",
    "    Extract code from triple-backtick fences.\n",
    "    \"\"\"\n",
    "    code_pattern = r\"```(?:\\w+)?\\s*\\n(.*?)```\"\n",
    "    match = re.search(code_pattern, llm_output, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def save_code_to_file(code, filename):\n",
    "    \"\"\"\n",
    "    Saves code content to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(code)\n",
    "\n",
    "def process_prompts_save_code(file_path, pipe_ref):\n",
    "    \"\"\"\n",
    "    Loads prompts from JSON, queries the LLM, extracts code, saves to .py files.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        instructions = json.load(f)\n",
    "\n",
    "    total = len(instructions)\n",
    "    code_blocks_saved = 0\n",
    "\n",
    "    for idx, instruction in enumerate(instructions, start=1):\n",
    "        prompt = instruction[\"instruction\"]\n",
    "        name = instruction.get(\"task_name\", f\"Task_{idx}\")\n",
    "        print(f\"[INFO] Generating code for '{name}'...\")\n",
    "\n",
    "        llm_output = query_llm(prompt, pipe_ref)\n",
    "        if not llm_output:\n",
    "            print(\"  [WARN] No LLM output.\")\n",
    "            continue\n",
    "\n",
    "        code_block = extract_code_from_reply(llm_output)\n",
    "        if code_block:\n",
    "            code_blocks_saved += 1\n",
    "            filename = f\"gen_code/generated_{idx}.py\"\n",
    "            save_code_to_file(code_block, filename)\n",
    "            print(f\"  [SAVED] {filename}\")\n",
    "        else:\n",
    "            print(\"  [WARN] No code block found.\")\n",
    "\n",
    "    print(\"\\n=== Summary of Phase 1 ===\")\n",
    "    print(f\"Total instructions: {total}\")\n",
    "    print(f\"Code blocks saved : {code_blocks_saved}\")\n",
    "    # Return or store relevant info if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating code for 'Basic BGP Update Collection'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_1.py\n",
      "[INFO] Generating code for 'Filtering by Specific ASN'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [WARN] No code block found.\n",
      "[INFO] Generating code for 'Extracting Unique Prefix Announcements'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [WARN] No code block found.\n",
      "[INFO] Generating code for 'Detecting Withdrawn Routes'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_4.py\n",
      "[INFO] Generating code for 'Monitoring AS Path Changes'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_5.py\n",
      "[INFO] Generating code for 'Analyzing BGP Prefix Announcements by Multiple ASNs'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_6.py\n",
      "[INFO] Generating code for 'Detecting Route Flapping Events'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_7.py\n",
      "[INFO] Generating code for 'Comparing AS Paths Between Different Route Collectors'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [WARN] No code block found.\n",
      "[INFO] Generating code for 'Identifying the Most Announced Prefixes'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_9.py\n",
      "[INFO] Generating code for 'Detecting MOAS (Multiple Origin AS) Conflicts'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_10.py\n",
      "[INFO] Generating code for 'Identifying Hijacked Prefixes'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [WARN] No code block found.\n",
      "[INFO] Generating code for 'Analyzing AS Path Prepending Behavior'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_12.py\n",
      "[INFO] Generating code for 'Detecting Sudden BGP Route Growth Events'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [WARN] No code block found.\n",
      "[INFO] Generating code for 'Tracking Longest AS Paths in Routing Data'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=712) and `max_length`(=712) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SAVED] generated_14.py\n",
      "[INFO] Generating code for 'Detecting BGP Route Leaks'...\n",
      "  [SAVED] generated_15.py\n",
      "\n",
      "=== Summary of Phase 1 ===\n",
      "Total instructions: 15\n",
      "Code blocks saved : 10\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"/home/hb/LLM-research/evaluation/BGP/BGP_analysis_test.json\"\n",
    "process_prompts_save_code(test_file_path, pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change env to python_39_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "\n",
    "def evaluate_python_file(filepath: str):\n",
    "    \"\"\"\n",
    "    Attempts to run the code in 'filepath'.\n",
    "    Returns a dict with 'status': 'pass'|'fail', plus optional 'error_message'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read in the code\n",
    "        with open(filepath, \"r\") as f:\n",
    "            code = f.read()\n",
    "\n",
    "        # Step 1: Check syntax\n",
    "        compile(code, filepath, \"exec\")\n",
    "\n",
    "        # Step 2: Execute\n",
    "        safe_globals = {\n",
    "            \"__builtins__\": __builtins__,\n",
    "            \"__name__\": \"__main__\",  # Avoid polluting actual environment\n",
    "        }\n",
    "        exec(code, safe_globals)\n",
    "\n",
    "        return {\"status\": \"pass\"}\n",
    "\n",
    "    except SyntaxError as se:\n",
    "        return {\n",
    "            \"status\": \"fail\",\n",
    "            \"error_message\": (f\"SyntaxError in {os.path.basename(filepath)}: {se.msg} \"\n",
    "                              f\"at line {se.lineno}, col {se.offset}\")\n",
    "        }\n",
    "    except Exception as ex:\n",
    "        return {\n",
    "            \"status\": \"fail\",\n",
    "            \"error_message\": (f\"RuntimeError in {os.path.basename(filepath)}:\\n\"\n",
    "                              f\"{traceback.format_exc()}\")\n",
    "        }\n",
    "\n",
    "def evaluate_all_generated_scripts(directory_path: str) -> None:\n",
    "    \"\"\"\n",
    "    PHASE 2:\n",
    "    Finds all .py files in 'directory_path' and runs them.\n",
    "    Summarizes pass/fail results for each script's code.\n",
    "    \"\"\"\n",
    "    all_py_files = [f for f in os.listdir(directory_path) if f.endswith(\".py\")]\n",
    "    total = len(all_py_files)\n",
    "    passed = 0\n",
    "    failed_files = []\n",
    "\n",
    "    for script_file in all_py_files:\n",
    "        filepath = os.path.join(directory_path, script_file)\n",
    "        print(f\"\\n[INFO] Evaluating: {script_file}\")\n",
    "\n",
    "        result = evaluate_python_file(filepath)\n",
    "        if result[\"status\"] == \"pass\":\n",
    "            print(f\"  [PASS] {script_file}\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"  [FAIL] {script_file}: {result['error_message']}\")\n",
    "            failed_files.append(script_file)\n",
    "\n",
    "    failed = total - passed\n",
    "    print(\"\\n=== Final Evaluation Summary ===\")\n",
    "    print(f\"Total scripts: {total}, Passed: {passed}, Failed: {failed}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(\"Failed scripts:\")\n",
    "        for ff in failed_files:\n",
    "            print(f\" - {ff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all_generated_scripts(\"gen_code/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_39_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
