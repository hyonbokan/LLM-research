{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\n",
      "Version: 0.45.0\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Author: Tim Dettmers\n",
      "Author-email: dettmers@cs.washington.edu\n",
      "License: MIT\n",
      "Location: /home/hb/.conda/envs/unsloth_env/lib/python3.10/site-packages\n",
      "Requires: numpy, torch, typing_extensions\n",
      "Required-by: unsloth\n",
      "---\n",
      "Name: peft\n",
      "Version: 0.14.0\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/hb/.local/lib/python3.10/site-packages\n",
      "Requires: accelerate, huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: unsloth, unsloth_zoo\n",
      "---\n",
      "Name: accelerate\n",
      "Version: 1.2.1\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/hb/.local/lib/python3.10/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: peft, trl, unsloth, unsloth_zoo\n",
      "---\n",
      "Name: transformers\n",
      "Version: 4.47.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/hb/.local/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, trl, unsloth, unsloth_zoo\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip show bitsandbytes peft accelerate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: OpenAI failed to import - ignoring for now.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.2.0. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 2.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.24. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_id = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    # load_in_4bit = load_in_4bit,\n",
    "    # token = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'most_similar_instructions', 'avg_similarity_score'],\n",
       "    num_rows: 2264\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"/home/hb/LLM-research/openai/generated_instructions/test_3.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:01<00:00, 138.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "CUTOFF_LEN = 2048\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Create the text prompt from your instruction, input, and output fields.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    \"\"\"\n",
    "    Tokenizes the prompt. Optionally pads to max_length=2048 and appends an EOS token.\n",
    "    Copies input_ids to labels for causal LM.\n",
    "    \"\"\"\n",
    "    # Here, we use padding=\"max_length\" to get uniform-length sequences of 2048.\n",
    "    # Alternatively, you can use padding=False and rely on a data collator.\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",   # or padding=False + data_collator\n",
    "        return_tensors=None,    # return raw Python lists\n",
    "    )\n",
    "\n",
    "    input_ids = result[\"input_ids\"]\n",
    "    attention_mask = result[\"attention_mask\"]\n",
    "\n",
    "    # Optionally place an EOS token at the very end if there's room\n",
    "    if (\n",
    "        add_eos_token\n",
    "        and len(input_ids) == CUTOFF_LEN\n",
    "        and input_ids[-1] != tokenizer.eos_token_id\n",
    "    ):\n",
    "        # Replace last token with EOS if you'd like\n",
    "        input_ids[-1] = tokenizer.eos_token_id\n",
    "        attention_mask[-1] = 1\n",
    "\n",
    "    labels = input_ids.copy()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Combines prompt generation with tokenization.\n",
    "    \"\"\"\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    return tokenize(full_prompt)\n",
    "\n",
    "# Example: split the \"train\" set into train/val\n",
    "train_val = data[\"train\"].train_test_split(test_size=200, shuffle=True, seed=42)\n",
    "train_data = train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    "val_data   = train_val[\"test\"].map(generate_and_tokenize_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "output_dir = \"/home/hb/dataset_bgp/BGP-LLaMA3-5k\"\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 1\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 200\n",
    "logging_steps = 500\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 10000\n",
    "warmup_ratio = 0.05\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = val_data,\n",
    "    dataset_text_field = \"output\",\n",
    "    logging_steps = 500,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        # warmup_steps = 5,\n",
    "        warmup_ratio = 0.05,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = max_steps,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_32bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/home/hb/dataset_bgp/BGP-LLaMA3-5k/outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,064 | Num Epochs = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 10,000\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n",
      "  5%|â–Œ         | 500/10000 [40:44<12:55:00,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.319, 'grad_norm': 0.1332283467054367, 'learning_rate': 0.0001, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1000/10000 [1:21:49<12:14:31,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1595, 'grad_norm': 0.11934316158294678, 'learning_rate': 9.931806517013612e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 1500/10000 [2:05:34<13:08:07,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1387, 'grad_norm': 0.10617193579673767, 'learning_rate': 9.729086208503174e-05, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 2000/10000 [2:51:06<10:54:36,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1223, 'grad_norm': 0.11179500818252563, 'learning_rate': 9.397368756032445e-05, 'epoch': 3.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 2500/10000 [3:32:14<10:12:30,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1058, 'grad_norm': 0.1258484572172165, 'learning_rate': 8.945702546981969e-05, 'epoch': 4.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [4:13:20<9:31:23,  4.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0885, 'grad_norm': 0.18343402445316315, 'learning_rate': 8.386407858128706e-05, 'epoch': 5.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3500/10000 [4:54:25<8:50:44,  4.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0726, 'grad_norm': 0.13948319852352142, 'learning_rate': 7.734740790612136e-05, 'epoch': 6.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4000/10000 [5:35:32<8:09:52,  4.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0574, 'grad_norm': 0.1467686891555786, 'learning_rate': 7.008477123264848e-05, 'epoch': 7.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4500/10000 [6:19:33<8:08:32,  5.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0443, 'grad_norm': 0.22314485907554626, 'learning_rate': 6.227427435703997e-05, 'epoch': 8.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [7:05:02<8:24:38,  6.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0336, 'grad_norm': 0.18578533828258514, 'learning_rate': 5.4128967273616625e-05, 'epoch': 9.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5500/10000 [7:50:40<6:41:50,  5.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0258, 'grad_norm': 0.16621412336826324, 'learning_rate': 4.5871032726383386e-05, 'epoch': 10.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6000/10000 [8:32:32<5:26:13,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0205, 'grad_norm': 0.08136153221130371, 'learning_rate': 3.772572564296005e-05, 'epoch': 11.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6500/10000 [9:13:26<4:45:59,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.017, 'grad_norm': 0.2654050886631012, 'learning_rate': 2.991522876735154e-05, 'epoch': 12.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [9:55:52<4:28:46,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.015, 'grad_norm': 0.04962505027651787, 'learning_rate': 2.2652592093878666e-05, 'epoch': 13.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7500/10000 [10:45:06<4:23:18,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0136, 'grad_norm': 0.057419996708631516, 'learning_rate': 1.6135921418712956e-05, 'epoch': 14.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8000/10000 [11:35:51<3:32:05,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0129, 'grad_norm': 0.048003681004047394, 'learning_rate': 1.0542974530180327e-05, 'epoch': 15.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8500/10000 [12:23:08<2:08:00,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0123, 'grad_norm': 0.04844491183757782, 'learning_rate': 6.026312439675552e-06, 'epoch': 16.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [13:07:33<1:27:12,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.012, 'grad_norm': 0.05417900159955025, 'learning_rate': 2.7091379149682685e-06, 'epoch': 17.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9500/10000 [13:51:41<43:39,  5.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0117, 'grad_norm': 0.05017364025115967, 'learning_rate': 6.819348298638839e-07, 'epoch': 18.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [14:32:57<00:00,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0114, 'grad_norm': 0.05167119577527046, 'learning_rate': 0.0, 'epoch': 19.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [14:33:02<00:00,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 52382.053, 'train_samples_per_second': 0.764, 'train_steps_per_second': 0.191, 'train_loss': 0.06469777879714966, 'epoch': 19.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"/home/hb/dataset_bgp/finetuned_models/LLaMA3-8B-instruct-analysis-10k-adam32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/hb/dataset_bgp/finetuned_models/LLaMA3-8B-instruct-analysis-10k-adam32/tokenizer_config.json',\n",
       " '/home/hb/dataset_bgp/finetuned_models/LLaMA3-8B-instruct-analysis-10k-adam32/special_tokens_map.json',\n",
       " '/home/hb/dataset_bgp/finetuned_models/LLaMA3-8B-instruct-analysis-10k-adam32/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.97s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer\n",
    ")\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the AS paths for each prefix associated with ASN AS4766 over the period oct 28 13:00 to oct 28 13:15, 2024. Provide minimum, maximum, and median AS path lengths and highlight any significant path changes observed in BGP updates. \n",
      "\n",
      "```python\n",
      "import os\n",
      "import re\n",
      "import pybgpstream\n",
      "import pandas as pd\n",
      "from datetime import datetime, timezone\n",
      "from collections import defaultdict\n",
      "\n",
      "def analyze_as_paths():\n",
      "    target_asn = \"4766\"\n",
      "    from_time_str = \"2024-10-28 13:00:00\"\n",
      "    until_time_str = \"2024-10-28 13:15:00\"\n",
      "    from_time = datetime.strptime(from_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n",
      "    until_time = datetime.strptime(until_time_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n",
      "    directory = \"/home/hb/routeviews_bgp_updates/2024/10/route-views2\"\n",
      "    pattern = r'^updates\\.(\\d{8})\\.(\\d{4})\\.(bz2|gz)$'\n",
      "    prefix_as_paths = defaultdict(list)\n",
      "    \n",
      "    for root, _, files in os.walk(directory):\n",
      "        for file in files:\n",
      "            match = re.match(pattern, file)\n",
      "            if match:\n",
      "                date_str = match.group(1)\n",
      "                time_str = match.group(2)\n",
      "                file_timestamp_str = date_str + time_str\n",
      "                file_time = datetime.strptime(file_timestamp_str, \"%Y%m%d%H%M\").replace(tzinfo=timezone.utc)\n",
      "                if file_time < from_time or file_time > until_time:\n",
      "                    continue\n",
      "                file_path = os.path.join(root, file)\n",
      "                stream = pybgpstream.BGPStream(data_interface=\"singlefile\")\n",
      "                stream.set_data_interface_option(\"singlefile\", \"upd-file\", file_path)\n",
      "                for rec in stream.records():\n",
      "                    for elem in rec:\n",
      "                        elem_time = datetime.utcfromtimestamp(elem.time).replace(tzinfo=timezone.utc)\n",
      "                        if elem_time < from_time or elem_time > until_time:\n",
      "                            continue\n",
      "                        if elem.type == \"A\":\n",
      "                            fields = elem.fields\n",
      "                            prefix = fields.get(\"prefix\")\n",
      "                            as_path_str = fields.get(\"as-path\", \"\")\n",
      "                            as_path = as_path_str.split()\n",
      "                            if as_path and as_path[-1] == target_asn:\n",
      "                                prefix_as_paths[prefix].append(as_path_str)\n",
      "    \n",
      "    as_path_lengths = []\n",
      "    for prefix, paths in prefix_as_paths.items():\n",
      "        lengths = [len(path.split()) for path in paths]\n",
      "        as_path_lengths.append({\n",
      "            \"prefix\": prefix,\n",
      "            \"min_length\": min(lengths),\n",
      "            \"max_length\": max(lengths),\n",
      "            \"median_length\": pd.Series(lengths).median()\n",
      "        })\n",
      "    \n",
      "    df_as_paths = pd.DataFrame(as_path_lengths)\n",
      "    if not df_as_paths.empty:\n",
      "        print(\"AS Path analysis for prefixes associated with ASN 4766:\")\n",
      "        print(df_as_paths)\n",
      "    else:\n",
      "        print(\"No AS path changes detected for ASN 4766 in the given time period.\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    analyze_as_paths()\n",
      "```\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1012)\n",
    "result = pipe(f\"Summarize the AS paths for each prefix associated with ASN AS4766 over the period oct 28 13:00 to oct 28 13:15, 2024. Provide minimum, maximum, and median AS path lengths and highlight any significant path changes observed in BGP updates.\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [22:56<00:00, 344.06s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hyonbokan/bgp-llama-3.1-instruct-10kSteps-2kDataset/commit/b08ba811e84206d23993e5ec24841de70e0aa72a', commit_message='Upload tokenizer', commit_description='', oid='b08ba811e84206d23993e5ec24841de70e0aa72a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hyonbokan/bgp-llama-3.1-instruct-10kSteps-2kDataset', endpoint='https://huggingface.co', repo_type='model', repo_id='hyonbokan/bgp-llama-3.1-instruct-10kSteps-2kDataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('hyonbokan/bgp-llama-3.1-instruct-10kSteps-2kDataset')\n",
    "tokenizer.push_to_hub('hyonbokan/bgp-llama-3.1-instruct-10kSteps-2kDataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
