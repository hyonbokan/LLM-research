{"id": "seed_task_0", "name": "Detect Multiple Origin AS (MOAS) Conflicts with PyBGPStream","instruction": "Generate a Python script using the PyBGPStream library to detect Multiple Origin AS (MOAS) conflicts in BGP data. The script should: Collect BGP updates for IPv4 prefixes during the time interval from '2020-01-01 00:00:00' to '2020-01-15 23:59:59' from the 'route-views' project. Identify MOAS conflicts where a prefix is announced by multiple origin ASes. Display the detected MOAS conflicts, indicating the prefix and the corresponding origin ASes.","instances": [    {        "input": "Python script using PyBGPStream to detect MOAS conflicts.",        "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime\n\nmoas_dict = defaultdict(set)\n\nstream = pybgpstream.BGPStream(from_time=\"2020-01-01 00:00:00\", until_time=\"2020-01-15 23:59:59\", project=\"route-views\", record_type=\"updates\", filter=\"ipversion 4\")\n\nfor rec in stream.records():\n    timestamp = datetime.utcfromtimestamp(rec.time)\n    for elem in rec:\n        if elem.type == \"A\":\n            origin_as = elem.fields['as-path'].split(\" \")[-1]\n            prefix = elem.fields['prefix']\n            moas_dict[prefix].add(origin_as)\n\nprint(\"MOAS conflicts detected:\")\nfor prefix, origins in moas_dict.items():\n    if len(origins) > 1:\n        print(f\"Prefix: {prefix}, Origin ASes: {', '.join(origins)}\")"    }],"is_classification": false}
{"id": "seed_task_1", "name": "BGP Anomaly Detection in AS Path Lengths", "instruction": "Perform BGP analysis and detect anomalies in AS path lengths for IPv4 prefixes over two time periods.", "instances": [{"input": "BGP anomaly detection in AS path lengths for IPv4 prefixes over two time periods.", "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime\nimport statistics\n\n# Data structure to hold average AS path lengths per prefix\naverage_path_lengths = defaultdict(list)\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-01-01 00:00:00\", until_time=\"2021-01-01 12:00:00\",\n    project=\"ris\",\n    collectors=\"rrc01,rrc02\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Collect average path lengths for later comparison\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":  # A for 'announcement'\n            as_path = elem.fields.get('as-path', '').split(\" \")\n            prefix = elem.fields.get('prefix', None)\n\n            if prefix and as_path:\n                path_length = len(as_path)\n                average_path_lengths[prefix].append(path_length)\n\n# Calculate average lengths per prefix\nfor prefix in average_path_lengths:\n    avg_length = statistics.mean(average_path_lengths[prefix])\n    average_path_lengths[prefix] = avg_length\n\n# Initialize a new stream for the period of interest\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-01-02 00:00:00\", until_time=\"2021-01-02 12:00:00\",\n    project=\"ris\",\n    collectors=\"rrc01,rrc02\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Detect anomalies in path lengths\nprint(\"Scanning for anomalies in AS path lengths...\")\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            as_path = elem.fields.get('as-path', '').split(\" \")\n            prefix = elem.fields.get('prefix', None)\n\n            if prefix and as_path and prefix in average_path_lengths:\n                path_length = len(as_path)\n                avg_length = average_path_lengths[prefix]\n\n                if path_length > avg_length * 1.5:  # Anomaly threshold\n                    print(f\"Anomaly detected for prefix {prefix}. Current length: {path_length}, Avg length: {avg_length}\")\n\n# The script can be extended to send alerts or store anomalous paths for further investigation."}], "is_classification": false}
{"id": "seed_task_2", "name": "Investigating BGP Community Local Preference Variations", "instruction": "Investigate variations in local preference values associated with BGP communities using data from collector rrc03 on July 15, 2021, between 10:00 and 11:00.", "instances": [{"input": "Investigating BGP community local preference variations using data from collector rrc03 on July 15, 2021, between 10:00 and 11:00.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-07-15 10:00:00\", until_time=\"2021-07-15 11:00:00\",\n    project=\"ris\",\n    collectors=\"rrc03\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Map of communities to local preference values\ncommunity_to_local_pref = defaultdict(list)\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            communities = elem.fields.get('communities', None)\n            local_pref = elem.fields.get('local_pref', None)\n\n            if communities and local_pref:\n                for community in communities.split(\",\"):\n                    community_to_local_pref[community].append(int(local_pref))\n\n# Calculate average local preference for each community\nprint(\"Average Local Preference Values for Communities:\")\nfor community, local_prefs in community_to_local_pref.items():\n    avg_local_pref = sum(local_prefs) / len(local_prefs)\n    print(f\"{community}: {avg_local_pref}\")"}], "is_classification": false}
{"id": "seed_task_3", "name": "Analyzing BGP Community Usage Trends", "instruction": "Analyze BGP community usage trends using data from collector rrc02 on June 1, 2021, between 00:00 and 01:00. Calculate the frequency of each BGP community and identify the top 5 most commonly used communities.", "instances": [{"input": "Analyzing BGP community usage trends using data from collector rrc02 on June 1, 2021, between 00:00 and 01:00.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-06-01 00:00:00\", until_time=\"2021-06-01 01:00:00\",\n    project=\"route-views\",\n    collectors=\"rrc02\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Counter for BGP communities\ncommunities_counter = Counter()\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            communities = elem.fields.get('communities', None)\n            if communities:\n                for community in communities.split(\",\"):\n                    communities_counter[community] += 1\n\n# Print the top 5 most commonly used communities\nprint(\"Top 5 Most Commonly Used Communities:\")\nfor comm, count in communities_counter.most_common(5):\n    print(f\"{comm}: {count}\")"}], "is_classification": false}
{"id": "seed_task_4", "name": "Identifying AS Path Prepending Usage", "instruction": "Identify Autonomous Systems (ASes) that potentially employ AS Path Prepending based on BGP data from collector route-views2 on January 15, 2021, between 00:00 and 01:00. Count and list ASes that engage in AS Path Prepending, along with the number of prepend events detected.", "instances": [{"input": "Identifying ASes employing AS Path Prepending based on BGP data from collector route-views2 on January 15, 2021, between 00:00 and 01:00.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-01-15 00:00:00\", until_time=\"2021-01-15 01:00:00\",\n    project=\"route-views\",\n    collectors=\"route-views2\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Store potential AS Path Prepending events\nas_path_prepend_events = defaultdict(int)\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            as_path = elem.fields.get('as-path', '').split()\n            as_count = defaultdict(int)\n\n            for as_number in as_path:\n                as_count[as_number] += 1\n\n            for as_number, count in as_count.items():\n                if count > 1:\n                    as_path_prepend_events[as_number] += 1\n\n# Display ASes employing AS Path Prepending\nprint(\"AS Employing Path Prepending:\")\nfor as_number, count in as_path_prepend_events.items():\n    print(f\"AS{as_number}: {count} prepend events\")"}], "is_classification": false}
{"id": "seed_task_5", "name": "Analyzing Prefix Announcements and Withdrawals", "instruction": "Analyze prefix announcements and withdrawals using BGP data from collector rrc04 on February 25, 2021, between 10:00 and 11:00. Count the frequency of announcements and withdrawals for each prefix and identify the top 5 most announced and withdrawn prefixes. Generate a report displaying the top 5 prefixes for both announcements and withdrawals along with their corresponding counts.", "instances": [{"input": "Analyzing prefix announcements and withdrawals using BGP data from collector rrc04 on February 25, 2021, between 10:00 and 11:00.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-02-25 10:00:00\", until_time=\"2021-02-25 11:00:00\",\n    project=\"ris\",\n    collectors=\"rrc04\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Counters for announcements and withdrawals\nannounce_counter = Counter()\nwithdraw_counter = Counter()\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        prefix = elem.fields.get('prefix', None)\n        if elem.type == \"A\":\n            announce_counter[prefix] += 1\n        elif elem.type == \"W\":\n            withdraw_counter[prefix] += 1\n\n# Print the top 5 most announced prefixes\nprint(\"Top 5 Most Announced Prefixes:\")\nfor prefix, count in announce_counter.most_common(5):\n    print(f\"{prefix}: {count} announcements\")\n\nprint(\"\\nTop 5 Most Withdrawn Prefixes:\")\nfor prefix, count in withdraw_counter.most_common(5):\n    print(f\"{prefix}: {count} withdrawals\")"}], "is_classification": false}
{"id": "seed_task_6", "name": "Identifying AS Peer Relationships", "instruction": "Identify Autonomous Systems (ASes) and their corresponding peer relationships using BGP data from collector route-views2 on May 15, 2021, between 00:00 and 01:00. Extract AS path information and determine which ASes are peers for each AS. Create a report listing the AS numbers along with their peers.", "instances": [{"input": "Identifying AS and their peer relationships using BGP data from collector route-views2 on May 15, 2021, between 00:00 and 01:00.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-05-15 00:00:00\", until_time=\"2021-05-15 01:00:00\",\n    project=\"route-views\",\n    collectors=\"route-views2\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Store AS Peer relationships\nas_peers = defaultdict(set)\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            as_path = elem.fields.get('as-path', '').split()\n            if len(as_path) > 1:\n                as_peers[as_path[-1]].add(as_path[-2])\n\n# Print ASes and their Peers\nprint(\"AS Peer Relationships:\")\nfor as_number, peers in as_peers.items():\n    print(f\"AS{as_number} has peers {', '.join(peers)}\")"}], "is_classification": false}
{"id": "seed_task_7", "name": "Identifying Top Hub ASes in BGP Paths", "instruction": "Identify the top hub Autonomous Systems (ASes) in BGP paths using data from collector rrc04 on July 20, 2021, between 10:00 and 11:00. Analyze AS path information and count the occurrence of each AS in the path to determine their hub status. Generate a report listing the top 5 hub ASes along with the number of times they appear in BGP paths.", "instances": [{"input": "Identifying top hub ASes in BGP paths using data from collector rrc04 on July 20, 2021, between 10:00 and 11:00.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-07-20 10:00:00\", until_time=\"2021-07-20 11:00:00\",\n    project=\"ris\",\n    collectors=\"rrc04\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Counter for AS hub\nas_hub_counter = Counter()\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            as_path = elem.fields.get('as-path', '').split()\n            as_hub_counter.update(as_path)\n\n# Print top 5 hub ASes\nprint(\"Top 5 Hub ASes:\")\nfor as_number, count in as_hub_counter.most_common(5):\n    print(f\"AS{as_number}: appears {count} times\")"}], "is_classification": false}
{"id": "seed_task_8", "name": "Analyzing Historical Trends in Unique Prefix Counts per Community","instruction": "Collect data for one-hour intervals over a period of three hours between 7:00 AM and 10:00 AM on 10th August 2015. For each one-hour interval, count the number of unique prefixes that are announced for each BGP community. Generate a summary of historical trends that shows how the number of unique prefixes for each community evolves over time.","instances": [    {        "input": "This script collects data for one-hour intervals over a period of three hours between 7:00 AM and 10:00 AM on 10th August 2015. For each one-hour interval, it counts the number of unique prefixes announced for each BGP community.",        "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\n# Initialize the time variables\nstart_time = datetime.strptime(\"2015-08-10 07:00:00\", \"%Y-%m-%d %H:%M:%S\")\nend_time = datetime.strptime(\"2015-08-10 10:00:00\", \"%Y-%m-%d %H:%M:%S\")\ntime_increment = timedelta(hours=1)\n\n# Data structures for storing prefix counts\nhistorical_prefix_counts = defaultdict(lambda: defaultdict(int))\n\nwhile start_time < end_time:\n    from_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    until_time_str = (start_time + time_increment).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    stream = pybgpstream.BGPStream(\n        from_time=from_time_str, until_time=until_time_str,\n        collectors=[\"rrc06\"],\n        record_type=\"ribs\",\n        filter=\"peer 25152 and prefix more 185.84.166.0/23 and community *:3400\"\n    )\n\n    current_prefix_count = defaultdict(int)\n    for rec in stream.records():\n        for elem in rec:\n            pfx = elem.fields['prefix']\n            communities = elem.fields['communities']\n            for community in communities:\n                current_prefix_count[community] += 1\n\n    # Save the data in the historical trends dictionary\n    for community, count in current_prefix_count.items():\n        historical_prefix_counts[community][from_time_str] = count\n\n    # Move to the next time slot\n    start_time += time_increment\n\n# Print the historical trends\nprint(\"Historical Trends in Unique Prefix Counts Per Community\")\nprint(\"-------------------------------------------------------\")\nfor community, time_series in historical_prefix_counts.items():\n    print(f\"Community: {community}\")\n    for timestamp, count in time_series.items():\n        print(f\"  {timestamp} -> {count} unique prefixes\")"}],"is_classification": false}
{"id": "seed_task_9", "name": "Detecting Withdrawal Spikes for Prefix '192.0.2.0/24'", "instruction": "Detect withdrawal spikes for the prefix '192.0.2.0/24' during May 5, 2022, from 12 PM to 8 PM. Identify time windows where the number of withdrawals exceeds 10 and print out the timestamps.", "instances": [{"input": "Detecting withdrawal spikes for the prefix '192.0.2.0/24' during May 5, 2022, from 12 PM to 8 PM.", "output": "from datetime import datetime\nimport pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-05-05 12:00:00\", until_time=\"2022-05-05 20:00:00\",\n    record_type=\"updates\",\n    filter='prefix exact 192.0.2.0/24'\n)\n\nwithdrawal_threshold = 10\nwithdrawal_window = 600  # 10 minutes in seconds\n\nwithdrawal_count = 0\ncurrent_window_start = None\n\nfor elem in stream:\n    if elem.type == 'W':\n        if current_window_start is None:\n            current_window_start = elem.time\n        withdrawal_count += 1\n\n    if current_window_start and elem.time - current_window_start >= withdrawal_window:\n        if withdrawal_count > withdrawal_threshold:\n            print(f\"Withdrawal spike detected in window starting at {datetime.utcfromtimestamp(current_window_start).strftime('%Y-%m-%d %H:%M:%S')}\")\n        withdrawal_count = 0\n        current_window_start = None"}], "is_classification": false}
{"id": "seed_task_10", "name": "Analyzing BGP Update Events from Collector 'rrc00'", "instruction": "Investigate BGP update events originating from the 'rrc00' collector, with the goal of analyzing patterns in announcement (A) messages over a specified period. Specifically, focus on announcements with AS path containing '65000' and '65500', within the timeframe from September 1, 2023, 8:00 AM to September 3, 2023, 11:59 PM. Identify trends and visualize the time distribution of these announcements.", "instances": [{"input": "Investigating BGP update events from the 'rrc00' collector, focusing on announcements with AS path containing '65000' and '65500', within the timeframe from September 1, 2023, 8:00 AM to September 3, 2023, 11:59 PM.", "output": "import pybgpstream\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nstream = pybgpstream.BGPStream(\n    project=\"routeviews\", collectors=\"rrc00\",\n    from_time=\"2023-09-01 08:00:00\", until_time=\"2023-09-03 23:59:59\",\n    record_type=\"updates\",\n    filter='path \"65000 65500\"'\n)\n\nannouncement_times = []\n\nfor elem in stream:\n    if elem.type == 'A':\n        announcement_times.append(elem.time)\n    print(datetime.utcfromtimestamp(elem.time).strftime('%Y-%m-%d %H:%M:%S'))\n\nprint(f\"Total number of announcements: {len(announcement_times)}\")\n\nif announcement_times:\n    time_bins = [i for i in range(int(min(announcement_times)), int(max(announcement_times)) + 3600, 3600)]\n    hist = plt.hist(announcement_times, bins=time_bins)\n\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Number of Announcements\")\n    plt.title(\"Time Distribution of Announcements\")\n    plt.show()"}], "is_classification": false}
{"id": "seed_task_11", "name": "Analyzing Withdrawn IPv6 Prefixes from 'rrc14' Collector", "instruction": "Retrieve BGP update events related to IPv6 prefixes originating from 'rrc14' collector during May 20, 2023, from 12:00 PM to 6:00 PM. Filter for withdrawals and identify the specific prefixes that have been withdrawn during this period. Present the prefixes in a readable format.", "instances": [{"input": "Retrieving BGP update events related to IPv6 prefixes originating from 'rrc14' collector during May 20, 2023, from 12:00 PM to 6:00 PM.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    project=\"routeviews\", collectors=\"rrc14\",\n    from_time=\"2023-05-20 12:00:00\", until_time=\"2023-05-20 18:00:00\",\n    record_type=\"updates\",\n    filter='ipv 6'\n)\n\nwithdrawn_prefixes = set()\n\nfor elem in stream:\n    if elem.type == 'W':\n        withdrawn_prefixes.add(elem.fields[\"prefix\"])\n\nprint(\"Withdrawn IPv6 Prefixes:\")\nfor prefix in withdrawn_prefixes:\n    print(prefix)"}], "is_classification": false}
{"id": "seed_task_12", "name": "Analyzing Routing Changes for '192.168' Prefixes from 'rrc22' Collector", "instruction": "Collect BGP update events from 'rrc22' collector and analyze changes in routing information for prefixes starting with '192.168'. Focus on the time range between July 10, 2023, 6:00 AM and July 11, 2023, 11:59 PM. Determine the most frequent AS path associated with announcements during this period.", "instances": [{"input": "Collecting BGP update events from 'rrc22' collector for prefixes starting with '192.168' during July 10, 2023, 6:00 AM to July 11, 2023, 11:59 PM.", "output": "import pybgpstream\nfrom collections import Counter\n\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc22\",\n    from_time=\"2023-07-10 06:00:00 UTC\",\n    until_time=\"2023-07-11 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\nannouncement_as_paths = []\n\nfor elem in stream:\n    if elem.type == \"A\":\n        prefix = elem.fields.get(\"prefix\")\n        if prefix and prefix.startswith(\"192.168\"):\n            as_path = elem.fields.get(\"as-path\")\n            if as_path:\n                announcement_as_paths.append(as_path)\n\nmost_common_as_path = Counter(announcement_as_paths).most_common(1)\n\nif most_common_as_path:\n    print(f\"The most frequent AS path for prefixes starting with 192.168 is: {most_common_as_path[0][0]}\")\nelse:\n    print(\"No relevant announcements found.\")"}], "is_classification": false}
{"id": "seed_task_13", "name": "Exploring BGP Announcements with Community '65001:123'", "instruction": "Explore BGP update events from the 'rrc00' and 'rrc04' collectors, focusing on announcements involving the community value '65001:123'. The analysis should cover the period from April 1, 2023, 12:00 PM to April 5, 2023, 11:59 PM. Identify the unique ASes that originated these announcements and list them.", "instances": [{"input": "Exploring BGP update events from 'rrc00' and 'rrc04' collectors for announcements with community '65001:123' during April 1, 2023, 12:00 PM to April 5, 2023, 11:59 PM.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    project=\"routeviews\", collectors=[\"rrc00\", \"rrc04\"],\n    from_time=\"2023-04-01 12:00:00\", until_time=\"2023-04-05 23:59:59\",\n    record_type=\"updates\",\n    filter='comm \"65001:123\"'\n)\n\noriginating_asns = set()\n\nfor elem in stream:\n    if elem.type == 'A':\n        originating_asns.add(elem.fields[\"as-path\"].split()[-1])\n\nprint(\"Unique ASes originating announcements with community '65001:123':\")\nfor asn in originating_asns:\n    print(asn)"}], "is_classification": false}
{"id": "seed_task_14", "name": "Analyzing BGP Announcements with AS Path '3356' and Prefix Length 24", "instruction": "Query BGP update events during June 15, 2023, from the 'rrc10' collector, focusing on announcements where the AS path includes AS 3356 and there's a prefix length of 24. Analyze the frequency of announcements per peer IP address during this time frame.", "instances": [{"input": "Query BGP update events from the 'rrc10' collector for announcements with AS path including '3356' and prefix length 24 during June 15, 2023. Analyze the frequency of announcements per peer IP address.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc10\",\n    from_time=\"2023-06-15 00:00:00 UTC\",\n    until_time=\"2023-06-15 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store the peer IP addresses for announcements\npeer_ips = []\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is an announcement ('A')\n    if elem.type == 'A':\n        as_path = elem.fields.get('as-path', '')\n        prefix = elem.fields.get('prefix', '')\n        \n        # Check if the AS path includes AS 3356 and the prefix length is 24\n        if '3356' in as_path.split() and prefix:\n            prefix_length = int(prefix.split('/')[-1])  # Extract length from 'X.X.X.X/length'\n            if prefix_length == 24:\n                peer_ip = elem.peer_address\n                if peer_ip:\n                    peer_ips.append(peer_ip)\n\n# Count the frequency of announcements per peer IP\npeer_ip_count = Counter(peer_ips)\n\n# Print the frequency of announcements per peer IP\nfor peer_ip, count in peer_ip_count.items():\n    print(f\"Peer IP: {peer_ip}, Number of Announcements: {count}\")\n\n# If there are no relevant announcements\nif not peer_ip_count:\n    print(\"No relevant announcements found.\")"}], "is_classification": false}
{"id": "seed_task_15", "name": "Analyzing IPv6 Withdrawals from Multiple BGP Collectors", "instruction": "Describe the changes in network reachability from July 1, 2023, to July 7, 2023, using BGP updates collected from both the 'rrc01' and 'route-views' collectors. Focus on the withdrawals of IPv6 prefixes and analyze the frequency of these withdrawals per collector.", "instances": [{"input": "Analyze BGP updates collected from 'rrc01' and 'route-views' collectors for withdrawals of IPv6 prefixes during the period from July 1, 2023, to July 7, 2023. Analyze the frequency of withdrawals per collector.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris,route-views\",\n    collectors=\"rrc01,route-views\",\n    from_time=\"2023-07-01 00:00:00 UTC\",\n    until_time=\"2023-07-07 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store collector-specific IPv6 withdrawals\nwithdrawals_per_collector = {}\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is a withdrawal ('W') and IPv6\n    if elem.type == 'W' and elem.fields.get('ip-version') == '6':\n        collector = elem.collector\n        if collector not in withdrawals_per_collector:\n            withdrawals_per_collector[collector] = 0\n        withdrawals_per_collector[collector] += 1\n\n# Print the frequency of IPv6 withdrawals per collector\nfor collector, count in withdrawals_per_collector.items():\n    print(f\"Collector: {collector}, Number of IPv6 Withdrawals: {count}\")\n\n# If there are no withdrawals\nif not withdrawals_per_collector:\n    print(\"No IPv6 withdrawals found.\")"}], "is_classification": false}
{"id": "seed_task_16", "name": "Analyzing Frequent AS Paths in IPv4 Announcements", "instruction": "Investigate routing changes between June 1, 2023, and June 30, 2023, collected from the 'rrc03' collector. Identify the top 5 most frequent AS paths in announcements for IPv4 prefixes. Calculate the percentage of these top AS paths compared to the total number of announcements.", "instances": [{"input": "Analyze BGP updates collected from the 'rrc03' collector for announcements of IPv4 prefixes between June 1, 2023, and June 30, 2023. Identify the top 5 most frequent AS paths and calculate their percentages compared to the total number of announcements.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc03\",\n    from_time=\"2023-06-01 00:00:00 UTC\",\n    until_time=\"2023-06-30 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store AS paths for IPv4 announcements\nas_paths = []\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is an announcement ('A') and IPv4\n    if elem.type == 'A' and elem.fields.get('ip-version') == '4':\n        as_path = elem.fields.get('as-path', '')\n        as_paths.append(as_path)\n\n# Count the frequency of AS paths in announcements\nas_path_count = Counter(as_paths)\n\n# Get the top 5 most frequent AS paths\ntop_as_paths = as_path_count.most_common(5)\n\n# Calculate the total number of announcements\ntotal_announcements = len(as_paths)\n\n# Print the top AS paths and their frequencies\nfor as_path, count in top_as_paths:\n    percentage = (count / total_announcements) * 100\n    print(f\"AS Path: {as_path}, Frequency: {count}, Percentage: {percentage:.2f}%\")"}], "is_classification": false}
{"id": "seed_task_17", "name": "Analyzing Increase in Announcements for Different Prefix Lengths", "instruction": "Retrieve BGP update events during August 15, 2023, from the 'rrc04' collector. Identify announcements for prefixes with a length of exactly 16 and 24. Determine the percentage increase in the number of announcements for prefixes with a length of 24 compared to those with a length of 16.", "instances": [{"input": "Retrieve BGP update events from the 'rrc04' collector for August 15, 2023. Identify announcements for prefixes with a length of 16 and 24. Calculate the percentage increase in the number of announcements for prefixes with length 24 compared to those with length 16.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc04\",\n    from_time=\"2023-08-15 00:00:00 UTC\",\n    until_time=\"2023-08-15 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store counts for prefixes with length 16 and 24\ncount_16 = 0\ncount_24 = 0\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is an announcement ('A')\n    if elem.type == 'A':\n        prefix = elem.fields.get('prefix', '')\n        if prefix:\n            prefix_length = int(prefix.split('/')[-1])  # Extract length from 'X.X.X.X/length'\n            if prefix_length == 16:\n                count_16 += 1\n            elif prefix_length == 24:\n                count_24 += 1\n\n# Calculate the percentage increase\nif count_16 > 0:\n    percentage_increase = ((count_24 - count_16) / count_16) * 100\nelse:\n    percentage_increase = 0\n\nprint(f\"Percentage Increase in Announcements (Prefix Length 16 to 24): {percentage_increase:.2f}%\")"}], "is_classification": false}
{"id": "seed_task_18", "name": "Analyzing Average Withdrawals per Day with AS 2914 in AS Path", "instruction": "Analyze BGP updates between September 1, 2023, and September 15, 2023, collected from the 'rrc05' collector. Focus on withdrawals with the AS path containing at least one instance of AS 2914. Calculate the average number of withdrawals per day during this period.", "instances": [{"input": "Analyze BGP updates from the 'rrc05' collector for withdrawals with AS path containing AS 2914. Focus on the period between September 1, 2023, and September 15, 2023. Calculate the average number of withdrawals per day during this period.", "output": "import pybgpstream\nfrom datetime import datetime, timedelta\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc05\",\n    from_time=\"2023-09-01 00:00:00 UTC\",\n    until_time=\"2023-09-15 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store withdrawal count and date information\nwithdrawal_count = 0\ncurrent_date = None\nday_counter = 0\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is a withdrawal ('W')\n    if elem.type == 'W':\n        as_path = elem.fields.get('as-path', '')\n        if '2914' in as_path.split():\n            withdrawal_count += 1\n\n        # Check if the date has changed\n        record_date = datetime.utcfromtimestamp(elem.time)\n        if current_date is None:\n            current_date = record_date\n        elif record_date.date() != current_date.date():\n            day_counter += 1\n            current_date = record_date\n\n# Calculate the average number of withdrawals per day\naverage_withdrawals_per_day = withdrawal_count / (day_counter + 1)\n\nprint(f\"Average Withdrawals per Day: {average_withdrawals_per_day:.2f}\")"}], "is_classification": false}
{"id": "seed_task_19", "name": "Analyzing Announcements with Specific Community Attribute", "instruction": "Collect BGP updates during October 10, 2023, from the 'rrc06' collector. Identify announcements with the community attribute containing '64500:200'. Calculate the percentage of these announcements compared to the total number of announcements during this day.", "instances": [{"input": "Collect BGP updates from the 'rrc06' collector for October 10, 2023. Identify announcements with the community attribute '64500:200'. Calculate the percentage of these announcements compared to the total number of announcements on that day.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc06\",\n    from_time=\"2023-10-10 00:00:00 UTC\",\n    until_time=\"2023-10-10 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store counts for announcements with the specific community attribute\ncount_community_64500_200 = 0\ntotal_announcements = 0\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is an announcement ('A')\n    if elem.type == 'A':\n        total_announcements += 1\n        community = elem.fields.get('communities', [])\n        if '64500:200' in community:\n            count_community_64500_200 += 1\n\n# Calculate the percentage of announcements with the specific community attribute\nif total_announcements > 0:\n    percentage_with_community = (count_community_64500_200 / total_announcements) * 100\nelse:\n    percentage_with_community = 0\n\nprint(f\"Percentage of Announcements with Community 64500:200: {percentage_with_community:.2f}%\")"}], "is_classification": false}
{"id": "seed_task_20", "name": "Analyzing MOAS Prefixes in BGP Update Records","instruction": "Retrieve BGP update records from the 'rrc01' collector for the time period between 2023-06-15 12:00:00 and 2023-06-15 14:00:00. Identify and extract MOAS prefixes during this time frame to explore potential multi-homing cases. Present the MOAS prefixes along with their corresponding AS path information.","instances": [{"input": "Retrieve BGP update records from the 'rrc01' collector between 2023-06-15 12:00:00 and 2023-06-15 14:00:00. Extract and identify MOAS prefixes during this period along with their AS path.",        "output": "Here are the identified MOAS prefixes and their corresponding AS paths:\nfrom collections import defaultdict\nimport pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-06-15 12:00:00\",\n    until_time=\"2023-06-15 14:00:00\",\n    collectors=[\"rrc01\"],\n    record_type=\"updates\"\n)\n\n# Dictionary to store prefix and its AS path\nprefix_aspath = defaultdict(list)\n\n# Fetch records and populate dictionary\nfor rec in stream.records():\n    for elem in rec:\n        if 'communities' in elem.fields and 'as-path' in elem.fields:\n            pfx = elem.fields['prefix']\n            as_path = elem.fields['as-path'].split(' ')\n            prefix_aspath[pfx] = as_path\n\n# Analyze MOAS prefixes\nmoas_prefixes = {}\n\nfor pfx, as_path in prefix_aspath.items():\n    if len(as_path) > 1:\n        moas_prefixes[pfx] = as_path\n\n# Display MOAS prefixes and AS paths\nprint('MOAS prefixes and their AS paths:')\nfor pfx, as_path in moas_prefixes.items():\n    print(f'Prefix: {pfx}, AS Path: {', '.join(as_path)}')\n"    }],"is_classification": false}
{"id": "seed_task_21", "name": "Analyzing Prefixes with Multiple AS Path Variations", "instruction": "Retrieve BGP RIB records from the 'rrc02' and 'route-views.sg' collectors for the time period between 2023-09-15 09:00:00 and 2023-09-15 12:00:00. Create a dictionary to store prefixes and their corresponding distinct AS path variations. Identify prefixes with multiple AS path variations (potential MOAS). Display these prefixes along with their distinct AS path variations.", "instances": [{"input": "Retrieve BGP RIB records from 'rrc02' and 'route-views.sg' collectors between 2023-09-15 09:00:00 and 2023-09-15 12:00:00. Create a dictionary to store prefixes and their distinct AS path variations. Identify prefixes with multiple AS path variations (potential MOAS).", "output": "Here are the prefixes with multiple AS path variations:\n\nfrom collections import defaultdict\nimport pybgpstream\n\nstream = pybgpstream.BGPStream(from_time=\"2023-09-15 09:00:00\", until_time=\"2023-09-15 12:00:00\", collectors=[\"rrc02\", \"route-views.sg\"], record_type=\"ribs\")\n\nprefix_as_paths = defaultdict(set)\n\nfor rec in stream.records():\n    for elem in rec:\n        pfx = elem.fields.get(\"prefix\")\n        as_path = elem.fields.get(\"as-path\")\n        if pfx and as_path:\n            ases = as_path.split(\" \")\n            prefix_as_paths[pfx].add(tuple(ases))\n\nmoas_prefixes = {pfx: paths for pfx, paths in prefix_as_paths.items() if len(paths) > 1}\n\nfor pfx, paths in moas_prefixes.items():\n    print(f\"Prefix: {pfx}, AS Path Variations: {paths}\")"}], "is_classification": false}
{"id": "seed_task_22", "name": "Geographical Analysis of MOAS Prefixes", "instruction": "Perform a geographical analysis on MOAS (Multiple Origin AS) prefixes to identify the geographical regions from which these prefixes are being announced. Time Frame: Use the BGP data from August 1, 2019, between 14:00:00 and 14:30:00 UTC. Collectors: Use rrc06 as the data collector. Data Type: Focus on BGP update messages. Geographical Data: Utilize the GeoLite2 database to fetch the geographical location of the origin ASes.", "instances": [{"input": "Perform geographical analysis on MOAS prefixes using BGP data from August 1, 2019, 14:00:00 to 14:30:00 UTC, collected by rrc06. Utilize GeoLite2 database for AS geographical locations.", "output": "{\"prefixes\": [\n{\"prefix\": \"192.0.2.0/24\", \"origin_as\": [\n{\"as\": \"AS12345\", \"countries\": [\"US\"]},\n{\"as\": \"AS54321\", \"countries\": [\"CA\"]}\n]},\n{\"prefix\": \"198.51.100.0/24\", \"origin_as\": [\n{\"as\": \"AS98765\", \"countries\": [\"DE\"]},\n{\"as\": \"AS54321\", \"countries\": [\"CA\"]}\n]}\n]}" }], "is_classification": false}
{"id": "seed_task_23", "name": "Time-Series Analysis of MOAS Events", "instruction": "Perform time-series analysis on the MOAS events from the 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Identify the timestamps when MOAS events are observed and the frequency of these events during this period. Plot this information on a time-series graph.", "instances": [{"input": "Perform time-series analysis on MOAS events from 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Identify timestamps of MOAS events and their frequencies. Plot on a time-series graph.", "output": "Here's the time-series analysis of MOAS events:\n\nfrom collections import defaultdict\nimport pybgpstream\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Initialize Time-Series dict\ntime_series_data = defaultdict(int)\n\n# Initialize the BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"updates\",\n)\n\n# Fetching BGP data and updating time-series\nfor rec in stream.records():\n    timestamp = datetime.utcfromtimestamp(rec.time).strftime('%Y-%m-%d %H:%M:%S')\n    time_series_data[timestamp] += 1\n\n# Plotting time-series data\nplt.plot(list(time_series_data.keys()), list(time_series_data.values()))\nplt.xlabel('Time')\nplt.ylabel('Number of MOAS Events')\nplt.show()"}], "is_classification": false}
{"id": "seed_task_24", "name": "Security Analysis for BGP Hijacking Attempts", "instruction": "Conduct a security analysis focused on identifying potential BGP hijacking attempts. Use data from the 'rrc00' collector for the time window between 7:50 am and 8:10 am UTC on August 1, 2015. Specifically, look for MOAS prefixes where multiple Origin ASes are involved. List out any suspicious Origin ASes for these MOAS prefixes as they could be potential hijack attempts.", "instances": [{"input": "Perform security analysis for potential BGP hijacking attempts using data from 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Identify MOAS prefixes with multiple suspicious Origin ASes.", "output": "Here are the suspicious MOAS prefixes with potential hijack attempts:\n\nfrom collections import defaultdict\nimport pybgpstream\n\n# Initialize the BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"updates\",\n)\n\n# Potential Hijack ASes\npotential_hijacks = defaultdict(set)\n\n# Process records\nfor rec in stream.records():\n    for elem in rec:\n        try:\n            pfx = elem.fields['prefix']\n            as_path = elem.fields['as-path'].split()\n            if as_path:\n                origin = as_path[-1]\n                potential_hijacks[pfx].add(origin)\n        except KeyError:\n            continue\n\n# Identify MOAS Prefixes with potential hijacks\nfor pfx, origins in potential_hijacks.items():\n    if len(origins) > 1:\n        print(f\"Suspicious MOAS Prefix: {pfx}, Origin ASes: {', '.join(origins)}\")"}], "is_classification": false}
{"id": "seed_task_25", "name": "Security Analysis for BGP Route Leaks Involving MOAS Prefixes", "instruction": "Conduct a security analysis to identify BGP route leaks involving MOAS prefixes. Use data from the 'rrc04' collector for the time window between 10:00 am and 10:20 am UTC on September 10, 2019. Specifically, identify MOAS prefixes where the multiple Origin ASes are not in a direct AS relationship (i.e., they are not provider-customer or peer-to-peer). List out the MOAS prefixes and any involved Origin ASes that could potentially be part of a route leak.", "instances": [{"input": "Perform security analysis to identify BGP route leaks involving MOAS prefixes using data from 'rrc04' collector between 10:00 am and 10:20 am UTC on September 10, 2019. Identify MOAS prefixes with non-direct AS relationships.", "output": "Here are the MOAS prefixes potentially involved in route leaks:\n\nfrom collections import defaultdict\nimport pybgpstream\n\n# Initialize the BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2019-09-10 10:00:00\", until_time=\"2019-09-10 10:20:00\",\n    collectors=[\"rrc04\"],\n    record_type=\"updates\",\n)\n\n# Dictionary to store potential route leaks\npotential_route_leaks = defaultdict(set)\n\n# Process records\nfor rec in stream.records():\n    for elem in rec:\n        try:\n            pfx = elem.fields['prefix']\n            as_path = elem.fields['as-path'].split()\n            if as_path:\n                origin = as_path[-1]\n                potential_route_leaks[pfx].add(origin)\n        except KeyError:\n            continue\n\n# Identify MOAS Prefixes with potential route leaks (this part would ideally use some AS relationship database)\nfor pfx, origins in potential_route_leaks.items():\n    if len(origins) > 1:\n        # Here you would actually check the AS relationships (Provider-Customer or Peer-to-Peer)\n        # For demonstration, we assume that these are not in a direct AS relationship\n        print(f\"Potential Route Leak MOAS Prefix: {pfx}, Origin ASes: {', '.join(origins)}\")"}], "is_classification": false}
{"id": "seed_task_26", "name": "AS Relationship Analysis for MOAS Prefixes", "instruction": "Determine the AS relationships involved in MOAS prefixes to understand if they belong to the same organization or different organizations. Use the 'rrc00' collector for data between 7:50 am and 8:10 am UTC on August 1, 2015. Filter based on AS path to include records that have at least one AS that is known to belong to a large organization (for example, AS 65501).", "instances": [{"input": "Perform AS relationship analysis for MOAS prefixes using data from 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Filter based on AS path including AS 65501.", "output": "Here are the AS relationships for MOAS prefixes:\n\nfrom collections import defaultdict\nimport pybgpstream\n\n# Initialize stream for AS relationships\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"updates\",\n    filter=\"path _65501_\"\n)\n\n# Create a dictionary to store MOAS prefixes and their origin ASes\nmoas_as_relationships = defaultdict(set)\n\nfor rec in stream.records():\n    for elem in rec:\n        if 'communities' in elem.fields and 'as-path' in elem.fields:\n            prefix = elem.fields['prefix']\n            as_path = elem.fields['as-path'].split(\" \")\n            origin_as = as_path[-1]\n            moas_as_relationships[prefix].add(origin_as)\n\n# Output the MOAS prefixes and their origin ASes\nfor prefix in moas_as_relationships:\n    if len(moas_as_relationships[prefix]) > 1:\n        print(f\"{prefix} -> {moas_as_relationships[prefix]}\")"}], "is_classification": false}
{"id": "seed_task_27", "name": "Community and Policy Analysis for MOAS Events", "instruction": "Perform a community and policy analysis to understand the implications of MOAS (Multiple Origin AS) events on BGP policy decisions. Use data collected from the 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Filter BGP records to only include IPv4 updates that carry community values (comm filter). Report on the BGP community values associated with MOAS prefixes and discuss the intent behind these community tags.", "instances": [{"input": "Perform community and policy analysis for MOAS events using data from 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Filter IPv4 updates with community values.", "output": "Here are the MOAS prefixes with associated community values:\n\nfrom collections import defaultdict\nimport pybgpstream\n\n# Initialize stream for community and policy analysis\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Create a dictionary to store MOAS prefixes and their community values\n# We'll use a list instead of a set for each prefix.\nmoas_communities = defaultdict(list)\n\nfor rec in stream.records():\n    for elem in rec:\n        if 'communities' in elem.fields and 'as-path' in elem.fields:\n            prefix = elem.fields['prefix']\n            community = elem.fields['communities']\n            as_path = elem.fields['as-path'].split(\" \")\n            origin_as = as_path[-1]\n            # Append the origin_as and community as a tuple to the list corresponding to each prefix\n            moas_communities[prefix].append((origin_as, community))\n\n# Output the MOAS prefixes along with their community values\nfor prefix in moas_communities:\n    if len(set(x[0] for x in moas_communities[prefix])) > 1:  # check the unique origin AS\n        print(f\"{prefix} -> {moas_communities[prefix]}\")"}], "is_classification": false}
{"id": "seed_task_28", "name": "In-Depth Path Length Analysis of BGP and Graph Representation", "instruction": "Perform an in-depth path length analysis to compare the shortest path lengths between peer and origin ASes in a real BGP environment and their respective shortest path lengths in an undirected graph representation. Use BGP RIBs collected from the 'rrc00' collector between 7:50:00 and 8:10:00 on August 1, 2015. Output the findings as a list of tuples containing (peer, origin, BGP_hop_length, shortest_graph_length).", "instances": [{"input": "Perform in-depth path length analysis to compare shortest path lengths between peer and origin ASes in a real BGP environment and undirected graph representation using BGP RIBs from 'rrc00' collector between 7:50:00 and 8:10:00 on August 1, 2015. Output findings as a list of tuples.", "output": "Here are the findings of the path length analysis:\n\nfrom collections import defaultdict\nfrom itertools import groupby\nimport networkx as nx\nimport pybgpstream\n\n# Initialize graph and BGP length map\nas_graph = nx.Graph()\nbgp_lens = defaultdict(lambda: defaultdict(lambda: None))\n\n# Initialize BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"ribs\"\n)\n\n# Populate graph and BGP length map\nfor rec in stream.records():\n    for elem in rec:\n        peer = str(elem.peer_asn)\n        hops = [k for k, _ in groupby(elem.fields['as-path'].split(\" \"))]\n        if len(hops) > 1 and hops[0] == peer:\n            origin = hops[-1]\n            for i in range(len(hops) - 1):\n                as_graph.add_edge(hops[i], hops[i+1])\n            bgp_lens[peer][origin] = min(list(filter(bool, [bgp_lens[peer][origin], len(hops)])))\n\n# Compare BGP hop length to graph shortest path\nfor peer in bgp_lens:\n    for origin in bgp_lens[peer]:\n        nxlen = len(nx.shortest_path(as_graph, source=peer, target=origin))\n        print((peer, origin, bgp_lens[peer][origin], nxlen))"}], "is_classification": false}
{"id": "seed_task_29", "name": "AS Relationship Analysis for Path Inflation", "instruction": "Analyze how different AS relationships contribute to path inflation. Create a map that assigns relationships (Peer, Customer, Provider) between AS pairs. Utilize the same data set from rrc00 between 10:50 and 11:00 on April 1, 2021. Provide a summary output listing each AS pair, their relationship type, and whether the BGP path was inflated compared to the graph shortest path.", "instances": [{"input": "Analyze AS relationships' contribution to path inflation using data from 'rrc00' collector between 10:50 and 11:00 on April 1, 2021. Create a map with AS relationships (Peer, Customer, Provider). Provide a summary output listing each AS pair, their relationship type, and inflation status.", "output": "Here's the analysis of AS relationships and path inflation:\n\nfrom collections import defaultdict\nfrom itertools import groupby\nimport networkx as nx\nimport pybgpstream\n\n# Initialize graph and BGP length map\nas_graph = nx.Graph()\nbgp_lens = defaultdict(lambda: defaultdict(lambda: None))\n\n# Initialize AS relationship map\nas_relationships = defaultdict(lambda: defaultdict(str))\n\n# Hardcoded AS relationships for demonstration (you would usually populate this from a database or file)\nas_relationships[\"AS64500\"][\"AS64501\"] = \"Provider\"\nas_relationships[\"AS64501\"][\"AS64502\"] = \"Customer\"\nas_relationships[\"AS64502\"][\"AS64503\"] = \"Peer\"\n\n# Initialize BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-04-01 10:50:00\", until_time=\"2021-04-01 11:00:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"ribs\"\n)\n\n# Populate graph and BGP length map\nfor rec in stream.records():\n    for elem in rec:\n        peer = str(elem.peer_asn)\n        hops = [k for k, _ in groupby(elem.fields['as-path'].split(\" \"))]\n        if len(hops) > 1 and hops[0] == peer:\n            origin = hops[-1]\n            for i in range(len(hops) - 1):\n                as_graph.add_edge(hops[i], hops[i + 1])\n            bgp_lens[peer][origin] = min(list(filter(bool, [bgp_lens[peer][origin], len(hops)])))\n\n# Initialize an output map to capture inflation due to relationship\nrelationship_inflation = defaultdict(set)\n\n# Assess the impact of relationships\nfor peer in bgp_lens:\n    for origin in bgp_lens[peer]:\n        try:\n            nxlen = len(nx.shortest_path(as_graph, source=peer, target=origin))\n            relationship = as_relationships[peer].get(origin, \"Unknown\")\n            inflated = \"Yes\" if bgp_lens[peer][origin] > nxlen else \"No\"\n\n            relationship_inflation[relationship].add((peer, origin, inflated))\n        except nx.NetworkXNoPath:\n            print(f\"No path exists in the graph between {peer} and {origin}. Skipping.\")\n\n# Print the result\nfor relationship in relationship_inflation:\n    print(f\"For relationship {relationship}, the following peer-origin pairs show inflation:\")\n    for peer, origin, inflated in relationship_inflation[relationship]:\n        print(f\"{peer} - {origin} : Inflated = {inflated}\")"}], "is_classification": false}
{"id": "seed_task_30", "name": "Advanced AS Relationship Analysis for Path Inflation", "instruction": "Analyze the effects of different AS relationshipsPeer, Customer, Provideron BGP path inflation. We'll use data collected from the rrc00 collector between 10:50 and 11:00 on April 1, 2021. Create a map that assigns the relationship between each AS pair, calculate the actual BGP path length and the graph-theoretical shortest path between each AS pair and output the list of each AS pair, their relationship type, and whether the BGP path is inflated compared to the graph's shortest path.", "instances": [{"input": "Analyze the effects of different AS relationshipsPeer, Customer, Provideron BGP path inflation using data from 'rrc00' collector between 10:50 and 11:00 on April 1, 2021. Create a map with AS relationships. Calculate BGP and graph-theoretical path lengths. Output each AS pair, their relationship type, and inflation status.", "output": "Here's the analysis of AS relationship effects on BGP path inflation:\n\nimport pybgpstream\nimport networkx as nx\nfrom collections import defaultdict\nfrom itertools import groupby\nfrom datetime import datetime\n\n# Create an instance of a simple undirected graph\nas_graph = nx.Graph()\n\n# Data structure to hold the shortest AS path lengths\nbgp_lens = defaultdict(lambda: defaultdict(lambda: None))\n\n# Data structure to hold time-based path lengths\ntime_based_bgp_lens = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n\n# Simulated event timeline\nevent_timeline = defaultdict(list)\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"ribs\",\n)\n\n# Populate the graph and path length data\nfor rec in stream.records():\n    timestamp = datetime.utcfromtimestamp(rec.time)\n    for elem in rec:\n        # Get the peer ASn\n        peer = str(elem.peer_asn)\n        \n        # Get the array of ASns in the AS path and remove repeatedly prepended ASns\n        hops = [k for k, g in groupby(elem.fields['as-path'].split(\" \"))]\n        \n        if len(hops) > 1 and hops[0] == peer:\n            # Get the origin ASn\n            origin = hops[-1]\n            \n            # Add new edges to the NetworkX graph\n            for i in range(0,len(hops)-1):\n                as_graph.add_edge(hops[i],hops[i+1])\n            \n            # Update the AS path length data\n            bgp_lens[peer][origin] = min(list(filter(bool,[bgp_lens[peer][origin],len(hops)])))\n            \n            # Update the time-based AS path length data\n            time_based_bgp_lens[timestamp][peer][origin] = \\\n                min(list(filter(bool,[time_based_bgp_lens[timestamp][peer][origin],len(hops)])))\n\n# Simulated event\nevent_timeline[\"2015-08-01 08:00:00\"].append(\"Outage at AS64501\")\n\n# Time-series analysis\nfor timestamp in sorted(time_based_bgp_lens.keys()):\n    total_inflation = 0\n    count = 0\n    \n    for peer in time_based_bgp_lens[timestamp]:\n        for origin in time_based_bgp_lens[timestamp][peer]:\n            try:\n                nxlen = len(nx.shortest_path(as_graph, peer, origin))\n                inflation = time_based_bgp_lens[timestamp][peer][origin] - nxlen\n                total_inflation += inflation\n                count += 1\n            except nx.NetworkXNoPath:\n                continue\n    \n    average_inflation = total_inflation / count if count > 0 else 0\n    \n    print(f\"Timestamp: {timestamp}, Average Path Inflation: {average_inflation}\")\n    \n    if timestamp.strftime(\"%Y-%m-%d %H:%M:%S\") in event_timeline:\n        for event in event_timeline[timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")]:\n            print(f\"  -> Event: {event}, Possible Inflation: {average_inflation}\")"}], "is_classification": false}
{"id": "seed_task_31", "name": "Event Correlation: BGP Community Transitions","instruction": "Track how prefixes move between different BGP communities during 7:00 AM to 9:00 AM on 1st August 2018. Print out transitions between communities during each 10-minute time slice, along with the count of such transitions. Transitions should be printed if a prefix changes its community.","instances": [{"input": "Track how prefixes move between different BGP communities during 7:00 AM to 9:00 AM on 1st August 2018. Print out transitions between communities during each 10-minute time slice, along with the count of such transitions. Transitions should be printed if a prefix changes its community.",        "output": "import pybgpstream\nfrom collections import defaultdict, Counter\nfrom datetime import datetime, timedelta\n\n# Initialize the starting and ending time\nstart_time = datetime.strptime(\"2018-08-01 07:00:00\", \"%Y-%m-%d %H:%M:%S\")\nend_time = datetime.strptime(\"2018-08-01 09:00:00\", \"%Y-%m-%d %H:%M:%S\")\n\n# Define the time increment for each interval (10 minutes)\ntime_increment = timedelta(minutes=10)\n\n# Data structures\nprefix_community_history = defaultdict(str)\ncommunity_transitions = defaultdict(Counter)\n\nwhile start_time < end_time:\n    # Format the start and end time of each interval\n    from_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    until_time_str = (start_time + time_increment).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Initialize BGPStream\n    stream = pybgpstream.BGPStream(\n        from_time=from_time_str, until_time=until_time_str,\n        collectors=[\"rrc06\"],\n        record_type=\"ribs\",\n        filter=\"peer 25152 and prefix more 185.84.166.0/23 and community *:3400\"\n    )\n\n    # Collect the data\n    for rec in stream.records():\n        for elem in rec:\n            pfx = elem.fields['prefix']\n            communities = elem.fields['communities']\n\n            # For simplification, let's consider only the first community if there are multiple\n            community = list(communities)[0] if communities else 'none'\n\n            # Check if the community has changed for this prefix\n            last_community = prefix_community_history.get(pfx, 'none')\n            if last_community != community:\n                community_transitions[last_community][community] += 1\n                prefix_community_history[pfx] = community\n\n    # Move to next time slot\n    start_time += time_increment\n\n# Output transitions as JSONL\nfor src_community, transitions in community_transitions.items():\n    for dst_community, count in transitions.items():\n        output = {\"source_community\": src_community, \"destination_community\": dst_community, \"transition_count\": count}\n        print(json.dumps(output))"}],"is_classification": false}
{"id": "seed_task_32", "name": "Monitoring Unique Prefixes for BGP Communities","instruction": "Monitor the number of unique prefixes for each BGP community between 7:00 AM and 9:00 AM on 10th August 2015. Trigger an alert if the number of unique prefixes for a community changes by more than 20% between two consecutive 10-minute intervals.","instances": [    {        "input": "This script monitors the number of unique prefixes for each BGP community between 7:00 AM and 9:00 AM on 10th August 2015. It triggers an alert if the number of unique prefixes for a community changes by more than 20% between two consecutive 10-minute intervals.",        "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\n# Initialize the time variables\nstart_time = datetime.strptime(\"2015-08-10 07:00:00\", \"%Y-%m-%d %H:%M:%S\")\nend_time = datetime.strptime(\"2015-08-10 09:00:00\", \"%Y-%m-%d %H:%M:%S\")\ntime_increment = timedelta(minutes=10)\n\n# Data structures for storing prefix counts\nprev_prefix_count = defaultdict(int)\ncurrent_prefix_count = defaultdict(int)\n\nwhile start_time < end_time:\n    from_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    until_time_str = (start_time + time_increment).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    stream = pybgpstream.BGPStream(\n        from_time=from_time_str, until_time=until_time_str,\n        collectors=[\"rrc06\"],\n        record_type=\"ribs\",\n        filter=\"peer 25152 and prefix more 185.84.166.0/23 and community *:3400\"\n    )\n\n    for rec in stream.records():\n        for elem in rec:\n            pfx = elem.fields['prefix']\n            communities = elem.fields['communities']\n            for community in communities:\n                current_prefix_count[community] += 1\n\n    # Check for alerts\n    for community in current_prefix_count.keys():\n        if community in prev_prefix_count:\n            prev_count = prev_prefix_count[community]\n            current_count = current_prefix_count[community]\n            percent_change = ((current_count - prev_count) / prev_count) * 100 if prev_count > 0 else 0\n\n            if abs(percent_change) > 20:\n                alert = {\"community\": community, \"prev_count\": prev_count, \"current_count\": current_count, \"percent_change\": percent_change}\n                print(json.dumps(alert))\n\n    # Update previous counts\n    prev_prefix_count = current_prefix_count.copy()\n    current_prefix_count.clear()\n\n    # Move to the next time slot\n    start_time += time_increment"}],"is_classification": false}
{"id": "seed_task_33", "name": "BGP Prefix and Community Analysis", "instruction": "Collect data for one-hour intervals over three hours between 7:00 AM and 10:00 AM on August 10, 2015. Count the number of unique prefixes announced for each BGP community. Generate a summary of historical trends showing how the number of unique prefixes for each community evolves over time.", "instances": [{"input": "Data collection and analysis for one-hour intervals over three hours.", "output": "Here are the historical trends for the number of unique prefixes for each BGP community:\n\nimport pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\nimport json\n\nstart_time = datetime.strptime(\"2015-08-10 07:00:00\", \"%Y-%m-%d %H:%M:%S\")\nend_time = datetime.strptime(\"2015-08-10 08:00:00\", \"%Y-%m-%d %H:%M:%S\")\n\ndata_structure = defaultdict(lambda: defaultdict(int))\n\nwhile start_time < end_time:\n    from_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    until_time_str = (start_time + timedelta(hours=1)).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    stream = pybgpstream.BGPStream(\n        from_time=from_time_str, until_time=until_time_str,\n        collectors=[\"rrc06\"],\n        record_type=\"updates\",\n        filter=\"peer 25152 and prefix more 185.84.166.0/23 and community *:3400\"\n    )\n\n    current_prefix_count = defaultdict(int)\n    for rec in stream.records():\n        for elem in rec:\n            pfx = elem.fields['prefix']\n            communities = elem.fields['communities']\n            for community in communities:\n                current_prefix_count[community] += 1\n\n    for community, count in current_prefix_count.items():\n        data_structure[community][from_time_str] = count\n\n    start_time += timedelta(hours=1)\n\noutput = []\nfor community, time_series in data_structure.items():\n    output.append({\"Community\": community, \"Prefix Counts\": time_series})\n\nfor entry in output:\n    print(json.dumps(entry))"}], "is_classification": false}
{"id": "seed_task_34", "name": "Real-time Anomaly Detection in BGP Routes", "instruction": "Write a Python script using the PyBGPStream library to detect real-time anomalies in BGP routes. Flag and alert when: A new AS path is observed for an existing prefix. A known prefix suddenly disappears from the stream.", "instances": [{"input": "Python script using PyBGPStream library for real-time BGP anomaly detection.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Initialize the stream\nstream = pybgpstream.BGPStream(\n    project=\"routeviews-stream\",\n    filter=\"router amsix\",\n)\n\ncurrent_as_paths = defaultdict(set)\n\nfor elem in stream:\n    if elem.type == \"A\":\n        pfx = elem.fields[\"prefix\"]\n        as_path = elem.fields[\"as-path\"]\n        if as_path not in current_as_paths[pfx]:\n            print(f\"Anomaly detected! New AS path {as_path} for prefix {pfx}\")\n            current_as_paths[pfx].add(as_path)\n    elif elem.type == \"W\":\n        pfx = elem.fields[\"prefix\"]\n        if pfx in current_as_paths:\n            print(f\"Anomaly detected! Prefix {pfx} withdrawn!\")\n            del current_as_paths[pfx]"}], "is_classification": false}
{"id": "seed_task_35", "name": "Real-Time Detection of Short-lived Prefix Announcements (Prefix Flapping)", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of Short-lived Prefix Announcements (Prefix Flapping). Detect when a prefix is announced and withdrawn repeatedly in a short time window (prefix flapping), which may indicate network instability or misconfiguration.", "instances": [{"input": "Python script using PyBGPStream library for real-time detection of prefix flapping.", "output": "import pybgpstream\nfrom collections import defaultdict\nimport time\n\nstream = pybgpstream.BGPStream(project=\"routeviews-stream\")\nflap_count = defaultdict(int)\ntimestamp_cache = defaultdict(int)\n\nfor elem in stream:\n    prefix = elem.fields.get('prefix', None)\n    if prefix:\n        current_time = int(time.time())\n        last_time = timestamp_cache.get(prefix, 0)\n        if current_time - last_time <= 60:\n            flap_count[prefix] += 1\n            if flap_count[prefix] > 5:\n                print(f\"Anomaly Detected! Prefix {prefix} is flapping.\")\n        else:\n            flap_count[prefix] = 0\n        timestamp_cache[prefix] = current_time"}], "is_classification": false}
{"id": "seed_task_36", "name": "Real-Time Detection of Subnet Changes", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of Subnet Changes. Detect when a larger prefix is suddenly divided into smaller subnets or vice versa, which may indicate potential re-allocation of IP space.", "instances": [{"input": "Python script using PyBGPStream library for real-time detection of subnet changes.", "output": "import pybgpstream\nfrom netaddr import IPNetwork\nfrom collections import defaultdict\n\nstream = pybgpstream.BGPStream(project=\"routeviews-stream\")\nknown_prefixes = defaultdict(set)\n\nfor elem in stream:\n    new_prefix = IPNetwork(elem.fields.get('prefix', '0.0.0.0/0'))\n    for known_prefix in known_prefixes:\n        if new_prefix in known_prefix or known_prefix in new_prefix:\n            print(f\"Anomaly Detected! Change in subnet: {known_prefix} -> {new_prefix}\")\n    known_prefixes[new_prefix] = True"}], "is_classification": false}
{"id": "seed_task_37", "name": "Real-Time Detection of BGP Session Resets", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of BGP Session Resets. Monitor for the mass withdrawal of routes from a single AS, which may indicate that the AS has reset its BGP session, possibly due to an issue.", "instances": [{"input": "Python script using PyBGPStream library for real-time detection of BGP session resets.", "output": "import pybgpstream\nfrom collections import defaultdict\n\nstream = pybgpstream.BGPStream(project=\"routeviews-stream\")\nwithdraw_count = defaultdict(int)\n\nfor elem in stream:\n    if elem.type == \"W\":\n        as_path = elem.fields.get('as-path', 'unknown')\n        withdraw_count[as_path] += 1\n\n        if withdraw_count[as_path] > 100:\n            print(f\"Anomaly Detected! Mass withdrawal from AS: {as_path}\")\n\n            # Reset count to avoid repetitive alerts\n            withdraw_count[as_path] = 0"}], "is_classification": false}
{"id": "seed_task_38", "name": "Investigation of Historical Trends in BGP Path Length", "instruction": "Investigate the historical trends in BGP path length using data collected from the rrc02 collector between 2023-01-01 and 2023-08-01. Perform a time-series analysis to calculate the average BGP path length for each month during this period. Visualize the results using a line chart and annotations indicating significant events that may have influenced path length changes.", "instances": [{"input": "Python script using PyBGPStream and Matplotlib for investigating historical trends in BGP path length.", "output": "import pybgpstream\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import defaultdict\nfrom datetime import datetime\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-01-01\", until_time=\"2023-08-01\",\n    collectors=\"rrc02\",\n    record_type=\"ribs\",\n)\n\nmonthly_path_lengths = defaultdict(list)\n\nfor rec in stream.records():\n    for elem in rec:\n        timestamp = datetime.fromtimestamp(rec.time)\n        path_length = len(elem.fields['as-path'].split())\n        month = timestamp.strftime(\"%Y-%m\")\n        monthly_path_lengths[month].append(path_length)\n\naverage_path_lengths = {month: sum(lengths) / len(lengths) for month, lengths in monthly_path_lengths.items()}\n\ndf = pd.DataFrame(list(average_path_lengths.items()), columns=['Month', 'Average Path Length'])\n\ndf['Month'] = pd.to_datetime(df['Month'])\n\ndf = df.sort_values('Month')\n\nplt.figure(figsize=(10, 6))\nplt.plot(df['Month'], df['Average Path Length'], marker='o')\nplt.xlabel(\"Month\")\nplt.ylabel(\"Average Path Length\")\nplt.title(\"Historical Trends in BGP Path Length\")\n\nplt.annotate(\"Event A\", xy=(pd.Timestamp('2023-03-01'), 6), xytext=(pd.Timestamp('2023-02-01'), 6.5),\n             arrowprops=dict(facecolor='black', arrowstyle=\"->\"),\n             )\n\nplt.annotate(\"Event B\", xy=(pd.Timestamp('2023-06-01'), 5), xytext=(pd.Timestamp('2023-06-20'), 5.5),\n             arrowprops=dict(facecolor='black', arrowstyle=\"->\"),\n             )\n\nplt.tight_layout()\nplt.show()"}], "is_classification": false}
{"id": "seed_task_39", "name": "Exploration of AS Relationship Influence on BGP Path Inflation", "instruction": "Explore the influence of various types of AS relationshipsPeer, Customer, and Provideron the inflation of BGP paths. Utilize BGP data gathered from collectors rrc01 and rrc02 between 14:00 and 15:00 on September 15, 2023. Construct a map to define the relationship type between each AS pair. Compute the effective BGP path length and the shortest path length for each AS pair in the graph. Output a comprehensive list of AS pairs, their associated relationship type, and indicate whether the BGP path is elongated compared to the graph's shortest path.", "instances": [{"input": "Python script using PyBGPStream and NetworkX for exploring AS relationship influence on BGP path inflation.", "output": "import pybgpstream\nimport networkx as nx\nfrom collections import defaultdict\nfrom datetime import datetime\n\nas_relationships = defaultdict(lambda: defaultdict(lambda: None))\nbgp_path_lengths = defaultdict(lambda: defaultdict(lambda: None))\nshortest_path_lengths = defaultdict(lambda: defaultdict(lambda: None))\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-09-15 14:00:00\", until_time=\"2023-09-15 15:00:00\",\n    collectors=\"rrc01,rrc02\",\n    record_type=\"ribs\",\n)\n\nas_graph = nx.DiGraph()\n\nfor rec in stream.records():\n    for elem in rec:\n        peer = str(elem.peer_asn)\n        origin = elem.fields['as-path'].split()[-1]\n        as_graph.add_edge(peer, origin)\n        if len(elem.fields['as-path'].split()) <= 2:\n            as_relationships[peer][origin] = \"Peer\"\n        elif peer == elem.fields['as-path'].split()[0]:\n            as_relationships[peer][origin] = \"Provider\"\n        else:\n            as_relationships[peer][origin] = \"Customer\"\n        bgp_path_lengths[peer][origin] = len(elem.fields['as-path'].split())\n        shortest_path_lengths[peer][origin] = nx.shortest_path_length(as_graph, peer, origin)\n\nfor peer in bgp_path_lengths:\n    for origin in bgp_path_lengths[peer]:\n        rel_type = as_relationships[peer][origin]\n        bgp_length = bgp_path_lengths[peer][origin]\n        shortest_length = shortest_path_lengths[peer][origin]\n        path_difference = bgp_length - shortest_length\n        print(f\"AS Pair: {peer} -> {origin}\")\n        print(f\"  Relationship: {rel_type}\")\n        print(f\"  BGP Path Length: {bgp_length}\")\n        print(f\"  Shortest Path Length: {shortest_length}\")\n        print(f\"  Path Inflation: {'Inflated' if path_difference > 0 else 'Not Inflated'}\")"}], "is_classification": false}
{"id": "seed_task_40", "name": "Investigation of Historical Trends in BGP Path Length with Network Events", "instruction": "Investigate the historical trends in BGP path length using data collected from the rrc02 collector between 2023-01-01 and 2023-08-01. Perform a time-series analysis to calculate the average BGP path length for each month during this period. Visualize the results using a line chart and annotations indicating significant events that may have influenced path length changes.", "instances": [{"input": "Python script using PyBGPStream to investigate historical trends in BGP path length with network events.", "output": "import pybgpstream\n\nnetwork_events = {\n    \"Event A\": \"2023-03-10 12:30:00\",\n    \"Event B\": \"2023-03-10 13:15:00\",\n    # Add more events and timestamps here\n}\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-03-10 12:00:00\", until_time=\"2023-03-10 14:00:00\",\n    collectors=\"rrc04\",\n    record_type=\"ribs\",\n)\n\nfor rec in stream.records():\n    timestamp = rec.time\n    for elem in rec:\n        prefix = elem.fields['prefix']\n        as_path = elem.fields['as-path'].split()\n        for event, event_time in network_events.items():\n            if abs((timestamp - pybgpstream.time.Time().from_str(event_time)).total_seconds()) <= 900:\n                print(f\"Event: {event}\")\n                print(f\"  Timestamp: {timestamp}\")\n                print(f\"  Prefix: {prefix}\")\n                print(f\"  AS Path: {as_path}\")\n                print(\"\\n\")"}], "is_classification": false}
{"id": "seed_task_41", "name": "Analysis of AS Path Preferences and AS Topology", "instruction": "Analyze the preferences of AS paths using data gathered from the collector rrc07 between 08:00 and 09:00 on May 20, 2023. Examine the most common AS paths and the frequency of their occurrence. Visualize the AS topology using a network graph. Output the list of top AS paths and their corresponding frequencies.", "instances": [{"input": "Python script using PyBGPStream, NetworkX, and Matplotlib to analyze AS path preferences and visualize AS topology.", "output": "import pybgpstream\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-05-20 08:00:00\", until_time=\"2023-05-20 09:00:00\",\n    collectors=\"rrc07\",\n    record_type=\"ribs\",\n)\n\nas_path_frequencies = defaultdict(int)\n\nfor rec in stream.records():\n    for elem in rec:\n        as_path = tuple(elem.fields['as-path'].split())\n        as_path_frequencies[as_path] += 1\n\n# Display top AS paths and their frequencies\ntop_paths = sorted(as_path_frequencies.items(), key=lambda x: x[1], reverse=True)[:10]\nfor path, frequency in top_paths:\n    print(f\"AS Path: {' -> '.join(path)}, Frequency: {frequency}\")\n\nas_graph = nx.Graph()\nfor as_path in as_path_frequencies:\n    for i in range(len(as_path) - 1):\n        as_graph.add_edge(as_path[i], as_path[i + 1])\n\nplt.figure(figsize=(10, 6))\npos = nx.spring_layout(as_graph, seed=42)\nnx.draw(as_graph, pos, with_labels=True, font_size=8, node_size=100, node_color='skyblue')\nplt.title(\"AS Topology\")\nplt.show()"}], "is_classification": false}
{"id": "seed_task_42", "name": "Real-Time Anomaly Detection in BGP Routes", "instruction": "Write a Python script using the PyBGPStream library to detect real-time anomalies in BGP (Border Gateway Protocol) routes. Flag and alert when: A new AS path is observed for an existing prefix. A known prefix suddenly disappears from the stream.", "instances": [{"input": "Python script using PyBGPStream for real-time anomaly detection in BGP routes.", "output": "import pybgpstream\nfrom collections import defaultdict\n\nstream = pybgpstream.BGPStream(\n    project=\"routeviews-stream\",\n    filter=\"router amsix\",\n)\n\ncurrent_as_paths = defaultdict(set)\n\nfor elem in stream:\n    if elem.type == \"A\":\n        pfx = elem.fields[\"prefix\"]\n        as_path = elem.fields[\"as-path\"]\n        if as_path not in current_as_paths[pfx]:\n            print(f\"Anomaly detected! New AS path {as_path} for prefix {pfx}\")\n            current_as_paths[pfx].add(as_path)\n    elif elem.type == \"W\":\n        pfx = elem.fields[\"prefix\"]\n        if pfx in current_as_paths:\n            print(f\"Anomaly detected! Prefix {pfx} withdrawn!\")\n            del current_as_paths[pfx]"}], "is_classification": false}
{"id": "seed_task_43", "name": "Real-Time Detection of Short-lived Prefix Announcements (Prefix Flapping)", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of Short-lived Prefix Announcements (Prefix Flapping). Detect when a prefix is announced and withdrawn repeatedly in a short time window (prefix flapping), which may indicate network instability or misconfiguration.", "instances": [{"input": "Python script using PyBGPStream for real-time detection of prefix flapping.", "output": "import pybgpstream\nfrom collections import defaultdict\nimport time\n\nstream = pybgpstream.BGPStream(project=\"routeviews-stream\")\nflap_count = defaultdict(int)\ntimestamp_cache = defaultdict(int)\n\nfor elem in stream:\n    prefix = elem.fields.get('prefix', None)\n    if prefix:\n        current_time = int(time.time())\n        last_time = timestamp_cache.get(prefix, 0)\n        if current_time - last_time <= 60:\n            flap_count[prefix] += 1\n            if flap_count[prefix] > 5:\n                print(f\"Anomaly Detected! Prefix {prefix} is flapping.\")\n        else:\n            flap_count[prefix] = 0\n        timestamp_cache[prefix] = current_time"}], "is_classification": false}
{"id": "seed_task_44", "name": "Real-Time Detection of Subnet Changes with Advanced Analysis", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of Subnet Changes. Detect when a larger prefix is suddenly divided into smaller subnets or vice versa, which may indicate potential re-allocation of IP space.", "instances": [{"input": "Python script using PyBGPStream for real-time detection of subnet changes with advanced analysis.", "output": "import pybgpstream\nimport networkx as nx\nfrom collections import defaultdict\nfrom datetime import datetime\n\nas_graph = nx.Graph()\nbgp_lens = defaultdict(lambda: defaultdict(lambda: None))\ntime_based_bgp_lens = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-01-01 00:00:00\", until_time=\"2022-06-30 23:59:59\",\n    project=\"route-views\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\nfor rec in stream.records():\n    timestamp = datetime.utcfromtimestamp(rec.time)\n    for elem in rec:\n        peer = str(elem.peer_asn)\n        hops = elem.fields['as-path'].split(\" \")\n        if len(hops) > 1 and str(hops[0]) == peer:\n            origin = str(hops[-1])\n            for i in range(0, len(hops) - 1):\n                as_graph.add_edge(str(hops[i]), str(hops[i + 1]))\n            bgp_lens[peer][origin] = min(list(filter(bool, [bgp_lens[peer][origin], len(hops)])))\n            time_based_bgp_lens[timestamp][peer][origin] = \\\n                min(list(filter(bool, [time_based_bgp_lens[timestamp][peer][origin], len(hops)])))\n\nfor timestamp in sorted(time_based_bgp_lens.keys()):\n    total_inflation = 0\n    count = 0\n    for peer in time_based_bgp_lens[timestamp]:\n        for origin in time_based_bgp_lens[timestamp][peer]:\n            try:\n                nxlen = len(nx.shortest_path(as_graph, peer, origin)) - 1\n                inflation = time_based_bgp_lens[timestamp][peer][origin] - nxlen\n                total_inflation += inflation\n                count += 1\n            except nx.NetworkXNoPath:\n                continue\n    average_inflation = total_inflation / count if count > 0 else 0\n    print(f\"Timestamp: {timestamp}, Average Path Inflation: {average_inflation}\")"}], "is_classification": false}
{"id": "seed_task_45", "name": "Analyzing BGP Updates for Peer 65001 and Prefix 192.0.2.0/24", "instruction": "Collect BGP data from the 'rrc06' collector within the time window of January 1, 2021, 00:00:00 to January 1, 2021, 01:00:00. Focus on the 'updates' record type with peer ASN 65001 and prefix 192.0.2.0/24.", "instances": [{"input": "BGP data collected from the 'rrc06' collector within the specified time window with the given filter.", "output": "Here is the collected BGP data:\n\nimport pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-01-01 00:00:00\",\n    until_time=\"2021-01-01 01:00:00\",\n    project=\"RIS\",\n    collectors=[\"rrc06\"],\n    record_type=\"updates\",\n    filter=\"type updates and collector rrc06 and peer 65001 and prefix 192.0.2.0/24\"\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_46", "name": "Analyzing BGP Update Patterns for Peer 32934", "instruction": "Analyze BGP update events related to peer 32934. Focus on announcements (A) and withdrawals (W) on October 10, 2021, from 3pm to 10pm. Count these events and identify patterns like quick withdrawals and spikes in event numbers.", "instances": [{"input": "BGP data collected from October 10, 2021, 3pm to 10pm, related to peer 32934.", "output": "from datetime import datetime\nimport math\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-10-04 15:00:00\", until_time=\"2021-10-04 22:00:00\",\n    record_type=\"updates\",\n    filter='peer 32934'\n)\nw_events = []\na_events = []\nOFFSET = 1633360000\n\n# ... (Rest of the output code) ..."}], "is_classification": false}
{"id": "seed_task_47", "name": "Retrieving IPv6 BGP Announcement Records", "instruction": "Retrieve BGP announcement records for IPv6 prefixes from the collector 'route-views.eqix'.", "instances": [{"input": "Retrieving BGP announcement records for IPv6 prefixes from the collector 'route-views.eqix'.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-01-01 00:00:00\", until_time=\"2022-01-01 01:00:00 UTC\",\n    collectors=[\"route-views.eqix\"],\n    filter=\"ipversion 6 and elemtype announcements\",\n    record_type=\"updates\"\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_48", "name": "Retrieving BGP Peer State Change Records", "instruction": "Retrieve BGP peer state change records for peer ASN 65500.", "instances": [{"input": "Retrieving BGP peer state change records for peer ASN 65500.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-01-01 00:00:00\", until_time=\"2022-01-01 01:00:00 UTC\",\n    filter=\"peer 65500 and elemtype peerstates\",\n    record_type=\"updates\"\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_49", "name": "Collecting BGP RIB Entries for IPv6 Prefixes with ASN 56789", "instruction": "Collect BGP RIB entries from a specific collector for IPv6 prefixes announced by AS path containing ASN 56789.", "instances": [{"input": "Collecting BGP RIB entries for IPv6 prefixes announced by AS path containing ASN 56789 from a specific collector.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-02-15 12:00:00\",\n    until_time=\"2023-02-15 13:00:00\",\n    collectors=[\"route-views.sg\"],\n    record_type=\"ribs\",\n    filter=\"type ribs and collector route-views.sg and ipversion 6 and path '_56789_'\",\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_50", "name": "Collecting BGP Updates for IPv4 Prefixes with Specific BGP Community", "instruction": "Collect BGP updates from a specific collector for IPv4 prefixes with a specific BGP community value.", "instances": [{"input": "Collecting BGP updates for IPv4 prefixes with a specific BGP community value from a specific collector.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-03-10 15:00:00\",\n    until_time=\"2023-03-10 16:00:00\",\n    collectors=[\"rrc10\"],\n    record_type=\"updates\",\n    filter=\"type updates and collector rrc10 and prefix more 0.0.0.0/0 and comm 12345:6789\"\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_51", "name": "Analyzing BGP Update Events for Prefix '203.0.113.0/24'", "instruction": "Examine the Border Gateway Protocol (BGP) update events for the prefix '203.0.113.0/24' during September 1, 2023, from 9 AM to 6 PM. Calculate the percentage of time the prefix was withdrawn and the average interval between announcements.", "instances": [{"input": "Analyzing BGP update events for the prefix '203.0.113.0/24' during September 1, 2023, from 9 AM to 6 PM.", "output": "from datetime import datetime, timedelta\nimport pybgpstream\n\nfrom_time_str = \"2023-09-01 09:00:00\"\nuntil_time_str = \"2023-09-01 18:00:00\"\nfrom_time = datetime.strptime(from_time_str, '%Y-%m-%d %H:%M:%S')\nuntil_time = datetime.strptime(until_time_str, '%Y-%m-%d %H:%M:%S')\n\ntotal_interval = (until_time - from_time).total_seconds()\n\nstream = pybgpstream.BGPStream(\n    from_time=from_time_str, until_time=until_time_str,\n    record_type=\"updates\",\n    filter='prefix exact 203.0.113.0/24'\n)\n\nwithdrawn_time = 0\nannouncement_intervals = []\nlast_announcement_time = None\n\nfor elem in stream:\n    if elem.type == 'W':\n        withdrawn_time += 1\n    elif elem.type == 'A':\n        if last_announcement_time:\n            interval = elem.time - last_announcement_time\n            announcement_intervals.append(interval)\n        last_announcement_time = elem.time\n\nif total_interval:\n    withdrawn_percentage = (withdrawn_time / total_interval) * 100\n    print(f\"Percentage of time prefix withdrawn: {withdrawn_percentage:.2f}%\")\n\nif announcement_intervals:\n    avg_interval = sum(announcement_intervals) / len(announcement_intervals)\n    print(f\"Average interval between announcements: {avg_interval:.2f} seconds\")"}], "is_classification": false}
{"id": "seed_task_52", "name": "Comparing BGP Update Events for Peers '65000' and '65100'", "instruction": "Compare BGP update events from peer '65000' and peer '65100' during November 15, 2022, from 2 PM to 8 PM. Determine if there are significant differences in the frequency of announcements and withdrawals between the two peers.", "instances": [{"input": "Comparing BGP update events from peers '65000' and '65100' during November 15, 2022, from 2 PM to 8 PM.", "output": "from datetime import datetime\nimport pybgpstream\n\npeers = ['65000', '65100']\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-11-15 14:00:00\", until_time=\"2022-11-15 20:00:00\",\n    record_type=\"updates\"\n)\n\npeer_data = {peer: {'announcements': 0, 'withdrawals': 0} for peer in peers}\n\nfor elem in stream:\n    peer_asn = str(elem.peer_asn)\n    if peer_asn not in peer_data:\n        continue\n    if elem.type == 'A':\n        peer_data[peer_asn]['announcements'] += 1\n    elif elem.type == 'W':\n        peer_data[peer_asn]['withdrawals'] += 1\n\nfor peer, data in peer_data.items():\n    print(f\"Peer {peer}:\")\n    print(f\"Number of announcements: {data['announcements']}\")\n    print(f\"Number of withdrawals: {data['withdrawals']}\")"}], "is_classification": false}
{"id": "seed_task_53", "name": "Analyzing BGP Update Events for AS Path Patterns '64512' and '65500'", "instruction": "Analyze BGP update events with AS path containing '64512' and '65500' during July 20, 2022, from 10 AM to 4 PM. Count the occurrences of different AS path patterns and identify the most common ones.", "instances": [{"input": "Analyzing BGP update events with AS path containing '64512' and '65500' during July 20, 2022, from 10 AM to 4 PM.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-07-20 10:00:00\", until_time=\"2022-07-20 16:00:00\",\n    record_type=\"updates\",\n    filter='path \"64512\" and path \"65500\"'\n)\n\nas_path_patterns = {}\n\nfor elem in stream:\n    as_path = elem.fields['as-path']\n    if as_path not in as_path_patterns:\n        as_path_patterns[as_path] = 1\n    else:\n        as_path_patterns[as_path] += 1\n\nsorted_patterns = sorted(as_path_patterns.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"Most common AS path patterns:\")\nfor pattern, count in sorted_patterns[:5]:\n    print(f\"Pattern: {pattern}, Count: {count}\")"}], "is_classification": false}
{"id": "seed_task_54", "name": "Detecting Hijacked Prefixes", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc00' collector. Identify prefixes where the observed origin AS differs from the expected origin AS provided in a dictionary. Report the prefixes with possible hijacks.", "instances": [{"input": "Collect BGP updates from the 'rrc00' collector for October 28, 2023. Use the expected origins: {'192.0.2.0/24': '64500', '198.51.100.0/24': '64501'}. Identify prefixes where the observed origin AS is different from the expected origin AS, and report these as possible hijacks.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc00"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nexpected_origins = {\n '192.0.2.0/24': '64500',\n '198.51.100.0/24': '64501',\n}\n\nhijacked_prefixes = set()\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n if prefix in expected_origins:\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n observed_origin = as_path[-1] if as_path else None\n expected_origin = expected_origins.get(prefix)\n if observed_origin != expected_origin:\n hijacked_prefixes.add(prefix)\n\n# Report possible hijacked prefixes\nprint("Possible hijacked prefixes:")\nfor prefix in hijacked_prefixes:\n print(f"{prefix}")"}], "is_classification": false}
{"id": "seed_task_55", "name": "Analyzing AS Path Prepending", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc01' collector. Identify ASes performing AS path prepending by detecting consecutive repeated ASNs in the AS paths. Report each ASN and the total number of times it has prepended.", "instances": [{"input": "Collect BGP updates from the 'rrc01' collector for October 28, 2023. Detect AS path prepending by finding consecutive repeated ASNs in the AS paths. Report each ASN and how many times it has performed prepending.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc01"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nprepending_counts = defaultdict(int)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n last_asn = None\n consecutive_count = 1\n for asn in as_path:\n if asn == last_asn:\n consecutive_count += 1\n else:\n if consecutive_count > 1 and last_asn is not None:\n prepending_counts[last_asn] += consecutive_count - 1\n consecutive_count = 1\n last_asn = asn\n # Check for prepending at the end of the path\n if consecutive_count > 1 and last_asn is not None:\n prepending_counts[last_asn] += consecutive_count - 1\n\n# Report ASNs performing AS path prepending\nprint("ASNs performing AS path prepending:")\nfor asn, count in prepending_counts.items():\n print(f"ASN {asn} prepended {count} times")"}], "is_classification": false}
{"id": "seed_task_56", "name": "Monitoring MOAS Conflicts", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc02' collector. Identify prefixes announced by multiple origin ASNs (MOAS conflicts). Report these prefixes along with the list of ASNs originating them.", "instances": [{"input": "Collect BGP updates from the 'rrc02' collector for October 28, 2023. Monitor for MOAS conflicts by identifying prefixes announced by more than one origin ASN. Report each conflicting prefix and the ASNs announcing it.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc02"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nprefix_origins = defaultdict(set)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n origin_asn = as_path[-1] if as_path else None\n if origin_asn and prefix:\n prefix_origins[prefix].add(origin_asn)\n\n# Report prefixes with MOAS conflicts\nprint("Prefixes with MOAS conflicts:")\nfor prefix, origins in prefix_origins.items():\n if len(origins) > 1:\n origins_list = ', '.join(origins)\n print(f"Prefix {prefix} announced by ASNs: {origins_list}")"}], "is_classification": false}
{"id": "seed_task_57", "name": "Calculating Average MED", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc03' collector. For all announcements that include a MED attribute, calculate the average MED value. Report the result.", "instances": [{"input": "Collect BGP updates from the 'rrc03' collector for October 28, 2023. Extract the MED values from announcements that have this attribute. Compute and report the average MED value.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc03"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nmed_values = []\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n med = elem.fields.get('med')\n if med is not None:\n try:\n med_values.append(int(med))\n except ValueError:\n pass # Ignore invalid MED values\n\n# Calculate and report the average MED value\nif med_values:\n average_med = sum(med_values) / len(med_values)\n print(f"Average MED value: {average_med}")\nelse:\n print("No MED values found in the announcements.")"}], "is_classification": false}
{"id": "seed_task_58", "name": "Counting Announcements and Withdrawals for Specific ASN", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc04' collector. Count the number of announcements and withdrawals that include ASN 3356 in their AS path. Report the counts.", "instances": [{"input": "Collect BGP updates from the 'rrc04' collector for October 28, 2023. Filter updates that have ASN '3356' in their AS path. Count and report the total number of announcements and withdrawals for this ASN.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc04"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\ntarget_asn = '3356'\nannouncements = 0\nwithdrawals = 0\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n if target_asn in as_path:\n if elem.type == 'A':\n announcements += 1\n elif elem.type == 'W':\n withdrawals += 1\n\n# Report the counts\nprint(f"Total announcements including ASN {target_asn}: {announcements}")\nprint(f"Total withdrawals including ASN {target_asn}: {withdrawals}")"}], "is_classification": false}
{"id": "seed_task_59", "name": "Detecting Route Leaks", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc05' collector. Identify potential route leaks by analyzing AS paths where a customer AS announces prefixes learned from one provider to another provider. Report these suspicious AS paths.", "instances": [{"input": "Collect BGP updates from the 'rrc05' collector for October 28, 2023. Analyze AS paths to detect potential route leaks where customer ASes are forwarding routes from one provider to another provider. Report the AS paths that may indicate route leaks.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# AS relationship data would normally be obtained from a dataset like CAIDA's AS relationships\n# For this example, we'll use a mock relationships dictionary\nas_relationships = {\n '100': {'providers': ['200', '300'], 'customers': ['400', '500']},\n '400': {'providers': ['100'], 'customers': []},\n # ... (additional AS relationships)\n}\n\ndef is_route_leak(as_path, as_relationships):\n # Simplified route leak detection\n for i in range(len(as_path) - 1):\n current_asn = as_path[i]\n next_asn = as_path[i + 1]\n if current_asn in as_relationships:\n if next_asn in as_relationships[current_asn].get('providers', []):\n # Customer is sending to provider, which is normal\n continue\n elif next_asn in as_relationships[current_asn].get('customers', []):\n # Provider is sending to customer, which is normal\n continue\n else:\n # Unexpected relationship, potential route leak\n return True\n return False\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc05"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n if as_path:\n if is_route_leak(as_path, as_relationships):\n print(f"Potential route leak detected: AS path {' -> '.join(as_path)}")"}], "is_classification": false}
{"id": "seed_task_60", "name": "Monitoring BGP Withdrawals for Outages", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc06' collector. Monitor for sustained withdrawals of prefixes without re-announcements to detect possible outages. Report the prefixes that remain withdrawn for more than 30 minutes.", "instances": [{"input": "Collect BGP updates from the 'rrc06' collector for October 28, 2023. Identify prefixes that have been withdrawn and not re-announced for over 30 minutes. Report these prefixes as possible outages.", "output": "import pybgpstream\nfrom datetime import datetime, timedelta\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc06"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nwithdrawals_timestamps = {}\noutage_threshold = timedelta(minutes=30)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n elem_time = datetime.utcfromtimestamp(elem.time)\n prefix = elem.fields.get('prefix')\n if prefix is None:\n continue\n if elem.type == 'W':\n withdrawals_timestamps[prefix] = elem_time\n elif elem.type == 'A':\n if prefix in withdrawals_timestamps:\n del withdrawals_timestamps[prefix]\n # Check for outages\n current_time = elem_time\n for withdrawn_prefix, withdrawal_time in list(withdrawals_timestamps.items()):\n if current_time - withdrawal_time > outage_threshold:\n print(f"Possible outage detected for {withdrawn_prefix} since {withdrawal_time}")\n del withdrawals_timestamps[withdrawn_prefix]"}], "is_classification": false}
{"id": "seed_task_61", "name": "Analyzing BGP Community Frequencies", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc07' collector. Analyze the frequency of different BGP community attributes used in the announcements. Report the top 10 most frequent community values.", "instances": [{"input": "Collect BGP updates from the 'rrc07' collector for October 28, 2023. Count the occurrences of each community attribute in the announcements. Report the top 10 community values by frequency.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc07"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\ncommunity_counts = defaultdict(int)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n communities = elem.fields.get('communities', [])\n for community in communities:\n community_str = f"{community[0]}:{community[1]}"\n community_counts[community_str] += 1\n\n# Get the top 10 communities by frequency\ntop_communities = sorted(community_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n\n# Report the top 10 communities\nprint("Top 10 BGP Community Values:")\nfor community, count in top_communities:\n print(f"{community}: {count} occurrences")"}], "is_classification": false}
{"id": "seed_task_62", "name": "Identifying the Top Origin ASNs", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc08' collector. Identify the top 5 origin ASNs announcing the most unique prefixes. Report these ASNs along with the number of unique prefixes they originated.", "instances": [{"input": "Collect BGP updates from the 'rrc08' collector for October 28, 2023. Determine the ASNs that originated the most unique prefixes. Report the top 5 ASNs and the count of unique prefixes they originated.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc08"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nasn_prefixes = defaultdict(set)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n origin_asn = as_path[-1] if as_path else None\n if origin_asn and prefix:\n asn_prefixes[origin_asn].add(prefix)\n\n# Calculate the number of unique prefixes per ASN\nasn_prefix_counts = {asn: len(prefixes) for asn, prefixes in asn_prefixes.items()}\n\n# Get the top 5 ASNs by unique prefix count\ntop_asns = sorted(asn_prefix_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n\n# Report the top 5 ASNs\nprint("Top 5 Origin ASNs by Unique Prefixes Announced:")\nfor asn, count in top_asns:\n print(f"ASN {asn}: {count} unique prefixes")"}], "is_classification": false}
{"id": "seed_task_63", "name": "Calculating the Average AS Path Length", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc09' collector. Calculate the average AS path length for all announcements received during the day. Report the average AS path length.", "instances": [{"input": "Collect BGP updates from the 'rrc09' collector for October 28, 2023. For each announcement, record the AS path length. Compute and report the average AS path length over all announcements.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc09"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\ntotal_path_length = 0\ntotal_announcements = 0\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n path_length = len(as_path)\n total_path_length += path_length\n total_announcements += 1\n\n# Calculate and report the average AS path length\nif total_announcements > 0:\n average_path_length = total_path_length / total_announcements\n print(f"Average AS Path Length: {average_path_length}")\nelse:\n print("No announcements found.")"}], "is_classification": false}
{"id": "seed_task_64", "name": "Detecting Flapping Prefixes", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc10' collector. Identify prefixes that have been announced and withdrawn more than 5 times during the day (flapping prefixes). Report these prefixes along with the number of times they flapped.", "instances": [{"input": "Collect BGP updates from the 'rrc10' collector for October 28, 2023. Monitor prefixes that are repeatedly announced and withdrawn. Report prefixes that have flapped more than 5 times and the count of flaps.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc10"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nflap_counts = defaultdict(int)\nprevious_states = {}\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n prefix = elem.fields.get('prefix')\n if prefix is None:\n continue\n current_state = elem.type\n last_state = previous_states.get(prefix)\n if last_state and last_state != current_state:\n flap_counts[prefix] += 1\n previous_states[prefix] = current_state\n\n# Identify flapping prefixes\nflapping_prefixes = {prefix: count for prefix, count in flap_counts.items() if count > 5}\n\n# Report flapping prefixes\nprint("Flapping prefixes:")\nfor prefix, count in flapping_prefixes.items():\n print(f"{prefix} flapped {count} times")"}], "is_classification": false}
{"id": "seed_task_65", "name": "Identifying BGP Updates with Specific MED Values", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc11' collector. Identify all announcements where the MED value is greater than 100. Report the prefixes and their corresponding MED values.", "instances": [{"input": "Collect BGP updates from the 'rrc11' collector for October 28, 2023. Find announcements with MED values greater than 100. Report the prefixes and their MED values.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc11"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n med = elem.fields.get('med')\n if med is not None:\n try:\n med_value = int(med)\n if med_value > 100:\n print(f"Prefix {prefix} has MED value {med_value}")\n except ValueError:\n pass # Ignore invalid MED values"}], "is_classification": false}
{"id": "seed_task_66", "name": "Analyzing Prefix Length Distribution", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc12' collector. Analyze the distribution of prefix lengths in the announcements. Report the number of prefixes for each prefix length.", "instances": [{"input": "Collect BGP updates from the 'rrc12' collector for October 28, 2023. Count the number of announced prefixes for each prefix length (e.g., /16, /24). Report the counts per prefix length.", "output": "import pybgpstream\nfrom collections import defaultdict\nimport ipaddress\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc12"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nprefix_length_counts = defaultdict(int)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n if prefix:\n try:\n network = ipaddress.ip_network(prefix)\n prefix_length = network.prefixlen\n prefix_length_counts[prefix_length] += 1\n except ValueError:\n pass # Invalid prefix\n\n# Report the prefix length distribution\nprint("Prefix Length Distribution:")\nfor length in sorted(prefix_length_counts.keys()):\n count = prefix_length_counts[length]\n print(f"/{length}: {count} prefixes")"}], "is_classification": false}
{"id": "seed_task_67", "name": "Monitoring BGP Updates from Specific Peer ASN", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc13' collector. Filter updates that are received from peer ASN 64496. Count the total number of announcements and withdrawals from this peer. Report the counts.", "instances": [{"input": "Collect BGP updates from the 'rrc13' collector for October 28, 2023. Focus on updates from peer ASN '64496'. Count and report the total number of announcements and withdrawals from this peer ASN.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc13"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\npeer_asn_filter = '64496'\nannouncements = 0\nwithdrawals = 0\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n peer_asn = elem.peer_asn\n if peer_asn == peer_asn_filter:\n if elem.type == 'A':\n announcements += 1\n elif elem.type == 'W':\n withdrawals += 1\n\n# Report the counts\nprint(f"Total announcements from peer ASN {peer_asn_filter}: {announcements}")\nprint(f"Total withdrawals from peer ASN {peer_asn_filter}: {withdrawals}")"}], "is_classification": false}
{"id": "seed_task_68", "name": "Identifying Updates with Large AS Paths", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc14' collector. Identify announcements with AS paths longer than 10 ASNs. Report the prefixes and their AS path lengths.", "instances": [{"input": "Collect BGP updates from the 'rrc14' collector for October 28, 2023. Find announcements where the AS path length exceeds 10. Report the prefixes and their AS path lengths.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project="ris",\n collectors=["rrc14"],\n from_time="2023-10-28 00:00:00 UTC",\n until_time="2023-10-28 23:59:59 UTC",\n record_type="updates",\n)\n\nthreshold = 10\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n path_length = len(as_path)\n if path_length > threshold:\n print(f"Prefix {prefix} has AS path length {path_length}")"}], "is_classification": false}
{"id": "seed_task_69", "name": "Monitoring BGP Announcements for Bogon Prefixes", "instruction": "Collect BGP updates on October 15, 2023, from the 'rrc00' collector. Identify announcements of bogon prefixes (e.g., 192.0.2.0/24, 198.51.100.0/24, 203.0.113.0/24). Report any occurrences of these announcements.", "instances": [{"input": "Collect BGP updates from the 'rrc00' collector for October 15, 2023. Identify any announcements of bogon prefixes such as '192.0.2.0/24', '198.51.100.0/24', and '203.0.113.0/24'. Report any instances where these prefixes are announced.", "output": "import pybgpstream\n\nbogon_prefixes = ['192.0.2.0/24', '198.51.100.0/24', '203.0.113.0/24']\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=[\"rrc00\"],\n    from_time=\"2023-10-15 00:00:00 UTC\",\n    until_time=\"2023-10-15 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Loop over the stream of BGP records\nprint(\"Bogon prefixes announced:\")\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            prefix = elem.fields.get('prefix')\n            if prefix in bogon_prefixes:\n                print(f\"Prefix {prefix} announced at {elem.time}\")"}], "is_classification": false}
{"id": "seed_task_70", "name": "Detecting BGP Communities Indicating Blackholing", "instruction": "Collect BGP updates on October 16, 2023, from the 'rrc01' collector. Identify announcements with BGP community attributes that indicate blackholing (e.g., '65535:666'). Report the prefixes associated with these communities.", "instances": [{"input": "Collect BGP updates from the 'rrc01' collector for October 16, 2023. Find announcements with the community attribute '65535:666' indicating blackholing. Report the prefixes that have this community attribute.", "output": "import pybgpstream\n\nblackhole_community = '65535:666'\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=[\"rrc01\"],\n    from_time=\"2023-10-16 00:00:00 UTC\",\n    until_time=\"2023-10-16 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Loop over the stream of BGP records\nprint(\"Prefixes with blackhole community:\")\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            communities = elem.fields.get('communities', [])\n            for community in communities:\n                community_str = f\"{community[0]}:{community[1]}\"\n                if community_str == blackhole_community:\n                    prefix = elem.fields.get('prefix')\n                    print(f\"Prefix {prefix} announced with blackhole community at {elem.time}\")"}], "is_classification": false}
{"id": "seed_task_71", "name": "Calculating the Number of Unique AS Paths", "instruction": "Collect BGP updates on October 17, 2023, from the 'rrc02' collector. For a specific prefix '1.1.1.0/24', calculate the number of unique AS paths observed during the day. Report the total count of unique AS paths.", "instances": [{"input": "Collect BGP updates from the 'rrc02' collector for October 17, 2023. Focus on the prefix '1.1.1.0/24' and record all unique AS paths seen. Calculate and report the number of unique AS paths for this prefix.", "output": "import pybgpstream\n\nprefix_of_interest = '1.1.1.0/24'\nunique_as_paths = set()\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=[\"rrc02\"],\n    from_time=\"2023-10-17 00:00:00 UTC\",\n    until_time=\"2023-10-17 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            prefix = elem.fields.get('prefix')\n            if prefix == prefix_of_interest:\n                as_path_str = elem.fields.get('as-path', '')\n                as_path = as_path_str.strip()\n                unique_as_paths.add(as_path)\n\n# Report the total number of unique AS paths\nprint(f\"Total unique AS paths for {prefix_of_interest}: {len(unique_as_paths)}\")"}], "is_classification": false}
{"id": "seed_task_72", "name": "Identifying Announcements with Invalid Origin AS", "instruction": "Collect BGP updates on October 18, 2023, from the 'rrc03' collector. Identify announcements where the origin AS is listed in the bogon ASNs (e.g., AS0, AS23456). Report the prefixes and their invalid origin ASNs.", "instances": [{"input": "Collect BGP updates from the 'rrc03' collector for October 18, 2023. Find announcements where the origin AS is a bogon ASN such as '0' or '23456'. Report the prefixes and their invalid origin ASNs.", "output": "import pybgpstream\n\nbogon_asns = {'0', '23456'}\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=[\"rrc03\"],\n    from_time=\"2023-10-18 00:00:00 UTC\",\n    until_time=\"2023-10-18 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Loop over the stream of BGP records\nprint(\"Announcements with invalid origin ASNs:\")\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            as_path_str = elem.fields.get('as-path', '')\n            as_path = as_path_str.strip().split()\n            origin_asn = as_path[-1] if as_path else None\n            if origin_asn in bogon_asns:\n                prefix = elem.fields.get('prefix')\n                print(f\"Prefix {prefix} announced with invalid origin ASN {origin_asn} at {elem.time}\")"}], "is_classification": false}
{"id": "seed_task_73", "name": "Analyzing BGP Announcements for Specific Prefix Lengths", "instruction": "Collect BGP updates on October 19, 2023, from the 'rrc04' collector. Identify and count announcements for prefixes longer than /24 (i.e., prefixes with prefix length greater than 24). Report the total count of such announcements.", "instances": [{"input": "Collect BGP updates from the 'rrc04' collector for October 19, 2023. Find announcements for prefixes with a prefix length greater than 24. Count and report the total number of these announcements.", "output": "import pybgpstream\nimport ipaddress\n\ntotal_announcements = 0\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=[\"rrc04\"],\n    from_time=\"2023-10-19 00:00:00 UTC\",\n    until_time=\"2023-10-19 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            prefix = elem.fields.get('prefix')\n            if prefix:\n                try:\n                    network = ipaddress.ip_network(prefix)\n                    if network.prefixlen > 24:\n                        total_announcements += 1\n                except ValueError:\n                    pass  # Skip invalid prefixes\n\n# Report the total count\nprint(f\"Total announcements for prefixes longer than /24: {total_announcements}\")"}], "is_classification": false}
{"id": "seed_task_74", "name": "Analyzing AS Path Changes for a Specific Prefix", "instruction": "Collect BGP updates on October 20, 2023, from the 'rrc05' collector. Monitor the AS path for the prefix '8.8.8.0/24' and report any changes observed during the day.", "instances": [{"input": "Collect BGP updates from the 'rrc05' collector for October 20, 2023. Focus on the prefix '8.8.8.0/24' and monitor for any AS path changes. Report all instances where the AS path for this prefix changes.", "output": "import pybgpstream\n\nprefix_of_interest = '8.8.8.0/24'\nprevious_as_path = None\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc05'],\n from_time='2023-10-20 00:00:00 UTC',\n until_time='2023-10-20 23:59:59 UTC',\n record_type='updates',\n)\n\nprint(f"Monitoring AS path changes for {prefix_of_interest}:")\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n if prefix == prefix_of_interest:\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip()\n if previous_as_path and as_path != previous_as_path:\n print(f"AS path changed for {prefix}: {previous_as_path} -> {as_path} at {elem.time}")\n previous_as_path = as_path"}], "is_classification": false}
{"id": "seed_task_75", "name": "Detecting BGP Hijacks for Multiple Prefixes", "instruction": "Collect BGP updates on October 21, 2023, from the 'rrc06' collector. Use a list of expected origin ASNs for specific prefixes. Identify any prefixes where the observed origin AS differs from the expected origin AS, and report them as possible hijacks.", "instances": [{"input": "Collect BGP updates from the 'rrc06' collector for October 21, 2023. Use the expected origins: {'10.0.0.0/24': '64512', '172.16.0.0/16': '64513'}. Report any prefixes where the observed origin AS is different from the expected.", "output": "import pybgpstream\n\nexpected_origins = {\n '10.0.0.0/24': '64512',\n '172.16.0.0/16': '64513',\n}\n\nhijacked_prefixes = set()\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc06'],\n from_time='2023-10-21 00:00:00 UTC',\n until_time='2023-10-21 23:59:59 UTC',\n record_type='updates',\n)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n if prefix in expected_origins:\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n observed_origin = as_path[-1] if as_path else None\n expected_origin = expected_origins.get(prefix)\n if observed_origin != expected_origin:\n hijacked_prefixes.add(prefix)\n\n# Report possible hijacked prefixes\nprint("Possible hijacked prefixes:")\nfor prefix in hijacked_prefixes:\n print(f"{prefix}")"}], "is_classification": false}
{"id": "seed_task_76", "name": "Analyzing BGP Communities for Traffic Engineering", "instruction": "Collect BGP updates on October 22, 2023, from the 'rrc07' collector. Identify announcements with BGP community attributes used for traffic engineering (e.g., '65000:80', '65000:90'). Report the prefixes and their associated communities.", "instances": [{"input": "Collect BGP updates from the 'rrc07' collector for October 22, 2023. Find announcements with community attributes '65000:80' or '65000:90' indicating traffic engineering. Report the prefixes and their communities.", "output": "import pybgpstream\n\ntraffic_engineering_communities = {'65000:80', '65000:90'}\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc07'],\n from_time='2023-10-22 00:00:00 UTC',\n until_time='2023-10-22 23:59:59 UTC',\n record_type='updates',\n)\n\nprint("Prefixes with traffic engineering communities:")\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n communities = elem.fields.get('communities', [])\n for community in communities:\n community_str = f"{community[0]}:{community[1]}"\n if community_str in traffic_engineering_communities:\n print(f"Prefix {prefix} has community {community_str}")"}], "is_classification": false}
{"id": "seed_task_77", "name": "Identifying Announcements with Private ASNs in AS Path", "instruction": "Collect BGP updates on October 23, 2023, from the 'rrc08' collector. Identify announcements where the AS path includes private ASNs (64512-65534). Report the prefixes and their AS paths containing private ASNs.", "instances": [{"input": "Collect BGP updates from the 'rrc08' collector for October 23, 2023. Find announcements where the AS path contains private ASNs. Report the prefixes and their AS paths with private ASNs.", "output": "import pybgpstream\n\nprivate_asns = set(str(asn) for asn in range(64512, 65535))\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc08'],\n from_time='2023-10-23 00:00:00 UTC',\n until_time='2023-10-23 23:59:59 UTC',\n record_type='updates',\n)\n\nprint("Announcements with private ASNs in AS path:")\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n as_path_str = elem.fields.get('as-path', '')\n as_path = as_path_str.strip().split()\n if any(asn in private_asns for asn in as_path):\n print(f"Prefix {prefix} has AS path {as_path_str} containing private ASNs")"}], "is_classification": false}
{"id": "seed_task_78", "name": "Monitoring BGP Announcements for Specific Next-Hop IP", "instruction": "Collect BGP updates on October 24, 2023, from the 'rrc09' collector. Identify announcements where the next-hop IP address is '192.0.2.1'. Report the prefixes associated with this next-hop IP.", "instances": [{"input": "Collect BGP updates from the 'rrc09' collector for October 24, 2023. Find announcements where the next-hop IP is '192.0.2.1'. Report the prefixes using this next-hop IP address.", "output": "import pybgpstream\n\nnext_hop_filter = '192.0.2.1'\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc09'],\n from_time='2023-10-24 00:00:00 UTC',\n until_time='2023-10-24 23:59:59 UTC',\n record_type='updates',\n)\n\nprint(f"Prefixes with next-hop IP {next_hop_filter}:")\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n next_hop = elem.fields.get('next-hop')\n if next_hop == next_hop_filter:\n print(f"Prefix {prefix} announced with next-hop IP {next_hop}")"}], "is_classification": false}
{"id": "seed_task_79", "name": "Calculating Average Local Preference", "instruction": "Collect BGP updates on October 25, 2023, from the 'rrc10' collector. For all announcements that include a Local Preference attribute, calculate the average Local Preference value. Report the result.", "instances": [{"input": "Collect BGP updates from the 'rrc10' collector for October 25, 2023. Extract the Local Preference values from announcements that have this attribute. Compute and report the average Local Preference.", "output": "import pybgpstream\n\nlocal_pref_values = []\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc10'],\n from_time='2023-10-25 00:00:00 UTC',\n until_time='2023-10-25 23:59:59 UTC',\n record_type='updates',\n)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n local_pref = elem.fields.get('local-pref')\n if local_pref is not None:\n try:\n local_pref_values.append(int(local_pref))\n except ValueError:\n pass # Ignore invalid values\n\n# Calculate and report the average Local Preference\nif local_pref_values:\n average_local_pref = sum(local_pref_values) / len(local_pref_values)\n print(f"Average Local Preference: {average_local_pref}")\nelse:\n print("No Local Preference values found in the announcements.")"}], "is_classification": false}
{"id": "seed_task_80", "name": "Analyzing BGP Updates for IPv6 Prefixes", "instruction": "Collect BGP updates on October 26, 2023, from the 'rrc11' collector. Identify and count announcements for IPv6 prefixes. Report the total number of IPv6 announcements.", "instances": [{"input": "Collect BGP updates from the 'rrc11' collector for October 26, 2023. Focus on announcements of IPv6 prefixes. Count and report the total number of IPv6 prefix announcements.", "output": "import pybgpstream\nimport ipaddress\n\ntotal_ipv6_announcements = 0\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project='ris',\n    collectors=['rrc11'],\n    from_time='2023-10-26 00:00:00 UTC',\n    until_time='2023-10-26 23:59:59 UTC',\n    record_type='updates',\n)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            prefix = elem.fields.get('prefix')\n            if prefix:\n                try:\n                    network = ipaddress.ip_network(prefix)\n                    if network.version == 6:\n                        total_ipv6_announcements += 1\n                except ValueError:\n                    pass  # Skip invalid prefixes\n\n# Report the total count\nprint(f\"Total IPv6 announcements: {total_ipv6_announcements}\")"}], "is_classification": false}
{"id": "seed_task_81", "name": "Monitoring BGP Updates for Specific AS Path Length", "instruction": "Collect BGP updates on October 27, 2023, from the 'rrc12' collector. Identify announcements where the AS path length is exactly 5. Report the prefixes and their AS paths.", "instances": [{"input": "Collect BGP updates from the 'rrc12' collector for October 27, 2023. Find announcements with AS path length equal to 5. Report the prefixes and their corresponding AS paths.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project='ris',\n    collectors=['rrc12'],\n    from_time='2023-10-27 00:00:00 UTC',\n    until_time='2023-10-27 23:59:59 UTC',\n    record_type='updates',\n)\n\npath_length_target = 5\n\n# Loop over the stream of BGP records\nprint(f\"Announcements with AS path length {path_length_target}:\")\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            prefix = elem.fields.get('prefix')\n            as_path_str = elem.fields.get('as-path', '')\n            as_path = as_path_str.strip().split()\n            if len(as_path) == path_length_target:\n                print(f\"Prefix {prefix} has AS path: {' -> '.join(as_path)}\")"}], "is_classification": false}
{"id": "seed_task_82", "name": "Identifying BGP Announcements with AS_SET in AS Path", "instruction": "Collect BGP updates on October 28, 2023, from the 'rrc13' collector. Identify announcements where the AS path includes AS_SET segments (denoted by curly braces '{}'). Report the prefixes and their AS paths.", "instances": [{"input": "Collect BGP updates from the 'rrc13' collector for October 28, 2023. Find announcements where the AS path contains AS_SET segments. Report the prefixes and their AS paths.", "output": "import pybgpstream\nimport re\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project='ris',\n    collectors=['rrc13'],\n    from_time='2023-10-28 00:00:00 UTC',\n    until_time='2023-10-28 23:59:59 UTC',\n    record_type='updates',\n)\n\n# Pattern to detect AS_SET\nas_set_pattern = re.compile(r'\\{.*?\\}')\n\nprint(\"Announcements with AS_SET in AS path:\")\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            prefix = elem.fields.get('prefix')\n            as_path_str = elem.fields.get('as-path', '')\n            if as_set_pattern.search(as_path_str):\n                print(f\"Prefix {prefix} has AS path with AS_SET: {as_path_str}\")"}], "is_classification": false}
{"id": "seed_task_83", "name": "Analyzing BGP Withdrawals Over Time", "instruction": "Collect BGP updates on October 29, 2023, from the 'rrc14' collector. Count the number of withdrawals per hour. Report the withdrawal counts for each hour of the day.", "instances": [{"input": "Collect BGP updates from the 'rrc14' collector for October 29, 2023. Tally the number of withdrawal messages received each hour. Provide a report showing the count of withdrawals for each hour.", "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project='ris',\n    collectors=['rrc14'],\n    from_time='2023-10-29 00:00:00 UTC',\n    until_time='2023-10-29 23:59:59 UTC',\n    record_type='updates',\n)\n\nwithdrawals_per_hour = defaultdict(int)\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'W':\n            elem_time = datetime.utcfromtimestamp(elem.time)\n            hour = elem_time.strftime('%Y-%m-%d %H:00')\n            withdrawals_per_hour[hour] += 1\n\n# Report the withdrawal counts per hour\nprint(\"Withdrawals per hour:\")\nfor hour in sorted(withdrawals_per_hour.keys()):\n    count = withdrawals_per_hour[hour]\n    print(f\"{hour}: {count} withdrawals\")"}], "is_classification": false}
{"id": "seed_task_84", "name": "Identifying BGP Announcements with Missing AS_PATH", "instruction": "Collect BGP updates on October 30, 2023, from the 'rrc15' collector. Identify announcements where the AS_PATH attribute is missing or empty. Report the prefixes with missing AS_PATH.", "instances": [{"input": "Collect BGP updates from the 'rrc15' collector for October 30, 2023. Find announcements that lack an AS_PATH attribute. Report the prefixes for which AS_PATH is missing or empty.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project='ris',\n    collectors=['rrc15'],\n    from_time='2023-10-30 00:00:00 UTC',\n    until_time='2023-10-30 23:59:59 UTC',\n    record_type='updates',\n)\n\nprint(\"Announcements with missing or empty AS_PATH:\")\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == 'A':\n            as_path_str = elem.fields.get('as-path')\n            if not as_path_str:\n                prefix = elem.fields.get('prefix')\n                print(f\"Prefix {prefix} has missing or empty AS_PATH\")"}], "is_classification": false}
{"id": "seed_task_85", "name": "Analyzing BGP Announcements for RPKI Invalids", "instruction": "Collect BGP updates on October 31, 2023, from the 'rrc00' collector. Identify announcements that are RPKI invalid, where the prefix and origin AS do not match the ROA records. Report the prefixes and origin ASNs.", "instances": [{"input": "Collect BGP updates from the 'rrc00' collector for October 31, 2023. Identify announcements that are RPKI invalid, where the prefix and origin AS do not match the ROA records. Report the prefixes and origin ASNs.", "output": "import pybgpstream\n\n# Placeholder function for RPKI validation\ndef is_rpki_invalid(prefix, origin_asn):\n # Implement RPKI validation logic here\n # Return True if the prefix and origin ASN combination is RPKI invalid\n # For demonstration purposes, this function always returns False\n return False\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc00'],\n from_time='2023-10-31 00:00:00 UTC',\n until_time='2023-10-31 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('RPKI Invalid Announcements:')\n\n# Loop over the stream of BGP records\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n prefix = elem.fields.get('prefix')\n as_path = elem.fields.get('as-path', '')\n if as_path:\n origin_asn = as_path.strip().split()[-1]\n if is_rpki_invalid(prefix, origin_asn):\n print(f'Prefix {prefix} with origin ASN {origin_asn} is RPKI invalid')"}], "is_classification": false}
{"id": "seed_task_86", "name": "Detecting Potential DDoS Attack Patterns", "instruction": "Collect BGP updates on November 1, 2023, from the 'rrc01' collector. Analyze the frequency of announcements and withdrawals for prefixes. Identify prefixes with sudden spikes in updates, indicating possible DDoS attack mitigations. Report these prefixes and time intervals.", "instances": [{"input": "Collect BGP updates from the 'rrc01' collector for November 1, 2023. Analyze the frequency of announcements and withdrawals for prefixes. Identify prefixes with sudden spikes in updates. Report these prefixes and time intervals.", "output": "import pybgpstream\nimport pandas as pd\nfrom datetime import datetime\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc01'],\n from_time='2023-11-01 00:00:00 UTC',\n until_time='2023-11-01 23:59:59 UTC',\n record_type='updates',\n)\n\n# Initialize data structures\ndata = []\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n timestamp = datetime.utcfromtimestamp(elem.time)\n prefix = elem.fields.get('prefix')\n data.append({'timestamp': timestamp, 'prefix': prefix, 'type': elem.type})\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Group by prefix and time intervals\ndf.set_index('timestamp', inplace=True)\nupdates_per_interval = df.groupby(['prefix', pd.Grouper(freq='15Min')]).size().reset_index(name='update_count')\n\n# Identify prefixes with spikes\nthreshold = updates_per_interval['update_count'].mean() + 3 * updates_per_interval['update_count'].std()\nanomalies = updates_per_interval[updates_per_interval['update_count'] > threshold]\n\n# Report anomalies\nprint('Prefixes with sudden spikes in updates:')\nfor index, row in anomalies.iterrows():\n print(f\"Prefix {row['prefix']} had {row['update_count']} updates at {row['timestamp']}\")"}], "is_classification": false}
{"id": "seed_task_87", "name": "Monitoring Prefix Deaggregation", "instruction": "Collect BGP updates on November 2, 2023, from the 'rrc02' collector. Identify cases where a prefix is announced with more specific sub-prefixes than usual. Report the prefixes and their sub-prefixes.", "instances": [{"input": "Collect BGP updates from the 'rrc02' collector for November 2, 2023. Identify prefixes announced with more specific sub-prefixes than usual. Report these prefixes and their sub-prefixes.", "output": "import pybgpstream\nfrom netaddr import IPSet, IPNetwork\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc02'],\n from_time='2023-11-02 00:00:00 UTC',\n until_time='2023-11-02 23:59:59 UTC',\n record_type='updates',\n)\n\n# Initialize data structures\nprefixes = {}\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n prefix = elem.fields.get('prefix')\n if prefix:\n supernet = str(IPNetwork(prefix).supernet())\n if supernet not in prefixes:\n prefixes[supernet] = set()\n prefixes[supernet].add(prefix)\n\n# Identify deaggregation cases\nprint('Prefixes with deaggregation:')\nfor supernet, subnets in prefixes.items():\n if len(subnets) > 1:\n print(f'Supernet: {supernet}')\n for subnet in subnets:\n print(f' Sub-prefix: {subnet}')}"], "is_classification": false}
{"id": "seed_task_88", "name": "Analyzing AS Relationships", "instruction": "Collect BGP updates on November 3, 2023, from the 'rrc03' collector. Build a graph of AS relationships based on the AS paths observed. Identify and report any new or unexpected AS relationships.", "instances": [{"input": "Collect BGP updates from the 'rrc03' collector for November 3, 2023. Build a graph of AS relationships from the AS paths. Identify and report new or unexpected AS relationships.", "output": "import pybgpstream\nimport networkx as nx\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc03'],\n from_time='2023-11-03 00:00:00 UTC',\n until_time='2023-11-03 23:59:59 UTC',\n record_type='updates',\n)\n\n# Initialize AS graph\nas_graph = nx.Graph()\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n as_path = elem.fields.get('as-path', '')\n if as_path:\n asns = as_path.strip().split()\n edges = zip(asns[:-1], asns[1:])\n as_graph.add_edges_from(edges)\n\n# Identify new relationships (placeholder logic)\n# In a real scenario, compare with a known baseline of AS relationships\nprint('AS Relationships:')\nfor edge in as_graph.edges():\n print(f'AS{edge[0]} - AS{edge[1]}')"}], "is_classification": false}
{"id": "seed_task_89", "name": "Detecting Unusually Long AS Paths", "instruction": "Collect BGP updates on November 4, 2023, from the 'rrc04' collector. Identify announcements with AS paths significantly longer than the average. Report the prefixes and their AS paths.", "instances": [{"input": "Collect BGP updates from the 'rrc04' collector for November 4, 2023. Identify announcements with AS paths longer than the average. Report the prefixes and their AS paths.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc04'],\n from_time='2023-11-04 00:00:00 UTC',\n until_time='2023-11-04 23:59:59 UTC',\n record_type='updates',\n)\n\n# Collect AS path lengths\nas_path_lengths = []\nannouncements = []\n\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n as_path = elem.fields.get('as-path', '')\n if as_path:\n length = len(as_path.strip().split())\n as_path_lengths.append(length)\n announcements.append({'prefix': elem.fields.get('prefix'), 'as_path': as_path, 'length': length})\n\n# Calculate average AS path length\naverage_length = sum(as_path_lengths) / len(as_path_lengths)\n\n# Identify unusually long AS paths\nthreshold = average_length + 3\nprint('Announcements with unusually long AS paths:')\nfor ann in announcements:\n if ann['length'] > threshold:\n print(f\"Prefix {ann['prefix']} with AS path {ann['as_path']}\")"}], "is_classification": false}
{"id": "seed_task_90", "name": "Monitoring BGP Updates for Specific Country Code", "instruction": "Collect BGP updates on November 5, 2023, from the 'rrc05' collector. Filter announcements where the origin AS is registered in a specific country, such as 'US'. Report the number of such announcements.", "instances": [{"input": "Collect BGP updates from the 'rrc05' collector for November 5, 2023. Filter announcements where the origin AS is registered in 'US'. Report the number of such announcements.", "output": "import pybgpstream\nimport requests\n\n# Function to get country code for an ASN\ndef get_country_code(asn):\n # Placeholder function\n # In practice, use a service or database to map ASN to country code\n return 'US' # For demonstration, assume all ASNs are 'US'\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc05'],\n from_time='2023-11-05 00:00:00 UTC',\n until_time='2023-11-05 23:59:59 UTC',\n record_type='updates',\n)\n\ncount = 0\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'A':\n as_path = elem.fields.get('as-path', '')\n if as_path:\n origin_asn = as_path.strip().split()[-1]\n country_code = get_country_code(origin_asn)\n if country_code == 'US':\n count += 1\n\nprint(f'Total announcements with origin AS in US: {count}')"}], "is_classification": false}
{"id": "seed_task_91", "name": "Identifying BGP Updates with Large Communities", "instruction": "Collect BGP updates on November 6, 2023, from the 'rrc06' collector. Identify announcements that include BGP Large Community attributes. Report the prefixes and their Large Communities.", "instances": [{"input": "Collect BGP updates from the 'rrc06' collector for November 6, 2023. Identify announcements with BGP Large Community attributes. Report the prefixes and their Large Communities.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc06'],\n from_time='2023-11-06 00:00:00 UTC',\n until_time='2023-11-06 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('Announcements with BGP Large Communities:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n communities = elem.fields.get('large-communities')\n if communities:\n prefix = elem.fields.get('prefix')\n print(f'Prefix {prefix} has Large Communities {communities}')"}], "is_classification": false}
{"id": "seed_task_92", "name": "Analyzing BGP AS Path Loops", "instruction": "Collect BGP updates on November 7, 2023, from the 'rrc07' collector. Identify AS paths that contain loops, where the same ASN appears more than once non-consecutively. Report the prefixes and AS paths.", "instances": [{"input": "Collect BGP updates from the 'rrc07' collector for November 7, 2023. Identify AS paths with loops. Report the prefixes and AS paths.", "output": "import pybgpstream\n\n# Function to check for AS path loops\ndef has_as_path_loop(as_path):\n asns = as_path.strip().split()\n seen = set()\n for asn in asns:\n if asn in seen:\n return True\n seen.add(asn)\n return False\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc07'],\n from_time='2023-11-07 00:00:00 UTC',\n until_time='2023-11-07 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('AS Paths with loops:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n as_path = elem.fields.get('as-path', '')\n if as_path and has_as_path_loop(as_path):\n prefix = elem.fields.get('prefix')\n print(f'Prefix {prefix} with AS path {as_path}')"}], "is_classification": false}
{"id": "seed_task_93", "name": "Monitoring BGP Announcements for Deprecated Attributes", "instruction": "Collect BGP updates on November 8, 2023, from the 'rrc08' collector. Identify announcements that include deprecated BGP attributes. Report the prefixes and the deprecated attributes used.", "instances": [{"input": "Collect BGP updates from the 'rrc08' collector for November 8, 2023. Identify announcements with deprecated BGP attributes. Report the prefixes and the attributes.", "output": "import pybgpstream\n\n# List of deprecated BGP attributes\ndeprecated_attributes = ['atomic-aggregate']\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc08'],\n from_time='2023-11-08 00:00:00 UTC',\n until_time='2023-11-08 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('Announcements with deprecated attributes:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n attributes = elem.fields\n deprecated_used = [attr for attr in deprecated_attributes if attr in attributes]\n if deprecated_used:\n prefix = elem.fields.get('prefix')\n print(f'Prefix {prefix} uses deprecated attributes {deprecated_used}')"}], "is_classification": false}
{"id": "seed_task_94", "name": "Calculating Prefix Reachability", "instruction": "Collect BGP updates on November 9, 2023, from the 'rrc09' collector. For a set of prefixes, determine the percentage of peers that have a route to each prefix. Report the prefixes and their reachability percentages.", "instances": [{"input": "Collect BGP updates from the 'rrc09' collector for November 9, 2023. Determine the percentage of peers with a route to each prefix. Report the prefixes and reachability percentages.", "output": "import pybgpstream\n\n# List of prefixes to check\nprefix_list = ['1.1.1.0/24', '8.8.8.0/24']\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc09'],\n from_time='2023-11-09 00:00:00 UTC',\n until_time='2023-11-09 23:59:59 UTC',\n record_type='rib',\n)\n\n# Initialize data structures\npeer_counts = {}\n\n# Collect data\nfor rec in stream.records():\n peer_asn = rec.peer_asn\n for elem in rec:\n prefix = elem.fields.get('prefix')\n if prefix in prefix_list:\n if prefix not in peer_counts:\n peer_counts[prefix] = set()\n peer_counts[prefix].add(peer_asn)\n\n# Total number of peers\ntotal_peers = len(set([rec.peer_asn for rec in stream.records()]))\n\n# Calculate reachability\nprint('Prefix Reachability Percentages:')\nfor prefix, peers in peer_counts.items():\n percentage = (len(peers) / total_peers) * 100 if total_peers > 0 else 0\n print(f'Prefix {prefix} is reachable by {percentage:.2f}% of peers')"}], "is_classification": false}
{"id": "seed_task_95", "name": "Detecting BGP Updates with Invalid NEXT_HOP", "instruction": "Collect BGP updates on November 10, 2023, from the 'rrc10' collector. Identify announcements where the NEXT_HOP attribute contains an invalid IP address. Report the prefixes and the invalid NEXT_HOP values.", "instances": [{"input": "Collect BGP updates from the 'rrc10' collector for November 10, 2023. Identify announcements with invalid NEXT_HOP IP addresses. Report the prefixes and NEXT_HOP values.", "output": "import pybgpstream\nimport ipaddress\n\n# Function to check if IP is valid\ndef is_valid_ip(ip):\n try:\n ipaddress.ip_address(ip)\n return True\n except ValueError:\n return False\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc10'],\n from_time='2023-11-10 00:00:00 UTC',\n until_time='2023-11-10 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('Announcements with invalid NEXT_HOP:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n next_hop = elem.fields.get('next-hop')\n if next_hop and not is_valid_ip(next_hop):\n prefix = elem.fields.get('prefix')\n print(f'Prefix {prefix} with invalid NEXT_HOP {next_hop}')"}], "is_classification": false}
{"id": "seed_task_96", "name": "Analyzing BGP Attributes for AS_PATH Aggregation", "instruction": "Collect BGP updates on November 11, 2023, from the 'rrc11' collector. Identify announcements where the AS_PATH indicates aggregation, such as the presence of AS_CONFED_SET. Report the prefixes and AS paths.", "instances": [{"input": "Collect BGP updates from the 'rrc11' collector for November 11, 2023. Identify announcements with AS_PATH aggregation indicators. Report the prefixes and AS paths.", "output": "import pybgpstream\nimport re\n\n# Regex pattern to detect AS_CONFED_SET or AS_SET (curly braces)\npattern = re.compile(r'\\{.*?\\}')\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc11'],\n from_time='2023-11-11 00:00:00 UTC',\n until_time='2023-11-11 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('Announcements indicating AS_PATH aggregation:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n as_path = elem.fields.get('as-path', '')\n if as_path and pattern.search(as_path):\n prefix = elem.fields.get('prefix')\n print(f'Prefix {prefix} with AS path {as_path}')"}], "is_classification": false}
{"id": "seed_task_97", "name": "Monitoring BGP Announcements for Specific Origin Validation State", "instruction": "Collect BGP updates on November 12, 2023, from the 'rrc12' collector. Identify announcements with an origin validation state of 'unknown'. Report the prefixes and origin ASNs.", "instances": [{"input": "Collect BGP updates from the 'rrc12' collector for November 12, 2023. Identify announcements with origin validation state 'unknown'. Report the prefixes and origin ASNs.", "output": "import pybgpstream\n\n# Configure BGPStream with origin validation (placeholder)\n# Note: Actual origin validation requires additional data\n\nprint('Announcements with origin validation state unknown:')\n\n# Since pybgpstream does not support origin validation natively,\n# we assume that the origin validation state is available\n# For demonstration purposes, this script will not perform real validation"}], "is_classification": false}
{"id": "seed_task_98", "name": "Identifying BGP Updates with Unusually High LOCAL_PREF", "instruction": "Collect BGP updates on November 13, 2023, from the 'rrc13' collector. Identify announcements where the LOCAL_PREF attribute is set higher than 1000. Report the prefixes and their LOCAL_PREF values.", "instances": [{"input": "Collect BGP updates from the 'rrc13' collector for November 13, 2023. Identify announcements with LOCAL_PREF higher than 1000. Report the prefixes and LOCAL_PREF values.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc13'],\n from_time='2023-11-13 00:00:00 UTC',\n until_time='2023-11-13 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('Announcements with LOCAL_PREF > 1000:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n local_pref = elem.fields.get('local-pref')\n if local_pref and int(local_pref) > 1000:\n prefix = elem.fields.get('prefix')\n print(f'Prefix {prefix} with LOCAL_PREF {local_pref}')"}], "is_classification": false}
{"id": "seed_task_99", "name": "Analyzing BGP Update Inter-arrival Times", "instruction": "Collect BGP updates on November 14, 2023, from the 'rrc14' collector. For a set of prefixes, calculate the inter-arrival times of updates. Identify prefixes with unusually short inter-arrival times. Report these prefixes.", "instances": [{"input": "Collect BGP updates from the 'rrc14' collector for November 14, 2023. Calculate inter-arrival times for updates. Identify prefixes with short inter-arrival times. Report these prefixes.", "output": "import pybgpstream\nfrom datetime import datetime\n\n# List of prefixes to monitor\nprefix_list = ['1.1.1.0/24', '8.8.8.0/24']\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc14'],\n from_time='2023-11-14 00:00:00 UTC',\n until_time='2023-11-14 23:59:59 UTC',\n record_type='updates',\n)\n\n# Initialize data structures\ntimestamps = {prefix: [] for prefix in prefix_list}\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n prefix = elem.fields.get('prefix')\n if prefix in prefix_list:\n timestamp = datetime.utcfromtimestamp(elem.time)\n timestamps[prefix].append(timestamp)\n\n# Calculate inter-arrival times\nprint('Prefixes with short inter-arrival times:')\nfor prefix, times in timestamps.items():\n times.sort()\n intervals = [(times[i+1] - times[i]).total_seconds() for i in range(len(times)-1)]\n if intervals:\n min_interval = min(intervals)\n if min_interval < 60: # Threshold in seconds\n print(f'Prefix {prefix} has a minimum inter-arrival time of {min_interval} seconds')"}], "is_classification": false}
{"id": "seed_task_100", "name": "Monitoring BGP Withdrawals for Specific ASN", "instruction": "Collect BGP updates on November 15, 2023, from the 'rrc15' collector. Identify withdrawals where ASN 64500 is present in the AS path before withdrawal. Report the prefixes withdrawn.", "instances": [{"input": "Collect BGP updates from the 'rrc15' collector for November 15, 2023. Identify withdrawals with ASN 64500 in the AS path. Report the prefixes withdrawn.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc15'],\n from_time='2023-11-15 00:00:00 UTC',\n until_time='2023-11-15 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('Withdrawals with ASN 64500 in AS path:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n if elem.type == 'W':\n as_path = elem.fields.get('as-path', '')\n if '64500' in as_path.split():\n prefix = elem.fields.get('prefix')\n print(f'Prefix {prefix} withdrawn with ASN 64500 in AS path')"}], "is_classification": false}
{"id": "seed_task_101", "name": "Detecting Announcements with Conflicting Origin ASNs", "instruction": "Collect BGP updates on November 16, 2023, from the 'rrc00' collector. For prefixes expected to be originated by a specific AS, identify any announcements where the origin ASN differs. Report the prefixes and observed origin ASNs.", "instances": [{"input": "Collect BGP updates from the 'rrc00' collector for November 16, 2023. For known prefixes, identify announcements where the origin ASN differs from expected. Report these prefixes and origin ASNs.", "output": "import pybgpstream\n\n# Expected origin ASNs for prefixes\nexpected_origins = {\n '1.1.1.0/24': '13335',\n '8.8.8.0/24': '15169',\n}\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc00'],\n from_time='2023-11-16 00:00:00 UTC',\n until_time='2023-11-16 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('Announcements with conflicting origin ASNs:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n prefix = elem.fields.get('prefix')\n if prefix in expected_origins:\n as_path = elem.fields.get('as-path', '')\n if as_path:\n origin_asn = as_path.strip().split()[-1]\n if origin_asn != expected_origins[prefix]:\n print(f'Prefix {prefix} expected origin {expected_origins[prefix]}, observed {origin_asn}')"}], "is_classification": false}
{"id": "seed_task_102", "name": "Analyzing BGP Updates for Specific AFI/SAFI", "instruction": "Collect BGP updates on November 17, 2023, from the 'rrc01' collector. Filter updates for a specific Address Family Identifier (AFI) and Subsequent Address Family Identifier (SAFI). Report the number of updates for each AFI/SAFI combination.", "instances": [{"input": "Collect BGP updates from the 'rrc01' collector for November 17, 2023. Filter updates for specific AFI/SAFI values. Report the number of updates for each combination.", "output": "import pybgpstream\n\n# Note: pybgpstream does not directly expose AFI/SAFI filtering\n# For demonstration, we will simulate the filtering process\n\nprint('Number of updates for each AFI/SAFI combination:')\n\n# Placeholder output\nprint('AFI 1/SAFI 1: 1000 updates')\nprint('AFI 2/SAFI 1: 500 updates')"}], "is_classification": false}
{"id": "seed_task_103", "name": "Monitoring BGP Announcements for New Prefixes", "instruction": "Collect BGP updates on November 18, 2023, from the 'rrc02' collector. Identify prefixes that have not been seen in the routing table before. Report these new prefixes and their origin ASNs.", "instances": [{"input": "Collect BGP updates from the 'rrc02' collector for November 18, 2023. Identify new prefixes not seen before. Report these prefixes and their origin ASNs.", "output": "import pybgpstream\n\n# Assume we have a set of known prefixes\nknown_prefixes = set()\n\n# Configure BGPStream for previous day to build known prefixes\nstream_prev = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc02'],\n from_time='2023-11-17 00:00:00 UTC',\n until_time='2023-11-17 23:59:59 UTC',\n record_type='rib',\n)\n\nfor rec in stream_prev.records():\n for elem in rec:\n prefix = elem.fields.get('prefix')\n if prefix:\n known_prefixes.add(prefix)\n\n# Configure BGPStream for the target day\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc02'],\n from_time='2023-11-18 00:00:00 UTC',\n until_time='2023-11-18 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('New prefixes announced:')\n\n# Collect data\nnew_prefixes = {}\nfor rec in stream.records():\n for elem in rec:\n prefix = elem.fields.get('prefix')\n if prefix and prefix not in known_prefixes:\n as_path = elem.fields.get('as-path', '')\n origin_asn = as_path.strip().split()[-1] if as_path else 'Unknown'\n new_prefixes[prefix] = origin_asn\n\nfor prefix, origin_asn in new_prefixes.items():\n print(f'Prefix {prefix} with origin ASN {origin_asn} is new')"}], "is_classification": false}
{"id": "seed_task_104", "name": "Detecting BGP Updates with Invalid AS_PATH Syntax", "instruction": "Collect BGP updates on November 19, 2023, from the 'rrc03' collector. Identify announcements where the AS_PATH attribute contains invalid syntax, such as non-numeric values. Report the prefixes and the invalid AS_PATH values.", "instances": [{"input": "Collect BGP updates from the 'rrc03' collector for November 19, 2023. Identify announcements with invalid AS_PATH syntax. Report the prefixes and AS_PATH values.", "output": "import pybgpstream\n\n# Function to check AS_PATH syntax\ndef is_valid_as_path(as_path):\n asns = as_path.strip().split()\n for asn in asns:\n if not asn.isdigit() and not (asn.startswith('{') and asn.endswith('}')):\n return False\n return True\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n project='ris',\n collectors=['rrc03'],\n from_time='2023-11-19 00:00:00 UTC',\n until_time='2023-11-19 23:59:59 UTC',\n record_type='updates',\n)\n\nprint('Announcements with invalid AS_PATH syntax:')\n\n# Collect data\nfor rec in stream.records():\n for elem in rec:\n as_path = elem.fields.get('as-path', '')\n if as_path and not is_valid_as_path(as_path):\n prefix = elem.fields.get('prefix')\n print(f'Prefix {prefix} with invalid AS_PATH {as_path}')"}], "is_classification": false}